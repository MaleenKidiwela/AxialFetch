{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hmean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import itertools\n",
    "import gsw\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(filepath):    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    data_df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Define constants for the number of header rows and the row indicating the start of data\n",
    "    NUM_HEADER_ROWS = 14\n",
    "    DATA_START_MARKER = \"# Data\"\n",
    "    \n",
    "    # Separate header rows and data rows\n",
    "    header_rows = data_df.iloc[:NUM_HEADER_ROWS]\n",
    "    data_rows = data_df[data_df[\"Code\"] != DATA_START_MARKER]\n",
    "    \n",
    "    # Create a dictionary to store DataFrames for each code\n",
    "    dataframes_dict = {}\n",
    "    \n",
    "    # Iterate through the header rows\n",
    "    for _, row in header_rows.iterrows():\n",
    "        # Extract the code and headers for this code\n",
    "        code, *headers = row.dropna().values\n",
    "        \n",
    "        # Rename the columns in a temporary DataFrame based on the extracted headers\n",
    "        rename_dict = {f\"V{i+1}\": header for i, header in enumerate(headers)}\n",
    "        temp_df = data_rows.copy()\n",
    "        temp_df.rename(columns=rename_dict, inplace=True)\n",
    "        \n",
    "        # Get the corresponding data rows based on the code\n",
    "        corresponding_data_rows = temp_df[temp_df[\"Code\"] == code]\n",
    "        \n",
    "        # If there are corresponding data rows:\n",
    "        if not corresponding_data_rows.empty:\n",
    "            # Set the first row as the header\n",
    "            corresponding_data_rows.columns = corresponding_data_rows.iloc[0].values\n",
    "            # Drop the first row (now header)\n",
    "            corresponding_data_rows = corresponding_data_rows.drop(corresponding_data_rows.index[0])\n",
    "            dataframes_dict[code] = corresponding_data_rows\n",
    "            \n",
    "    return dataframes_dict\n",
    "\n",
    "def create_nested_dictionary(filepaths):\n",
    "    \"\"\"\n",
    "    Process multiple files and return a dictionary containing dataframes for each file.\n",
    "    The keys of the outer dictionary are the last 4 digits of the csv file name.\n",
    "    \"\"\"\n",
    "    all_dfs = {}\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        # Extracting the identifier from the filename (using the last 4 digits)\n",
    "        identifier = filepath.split(\"_\")[-2]\n",
    "        \n",
    "        all_dfs[identifier] = process_data(filepath)\n",
    "        \n",
    "    return all_dfs\n",
    "\n",
    "# Check and convert to datetime if not already\n",
    "def ensure_datetime(df, column_name):\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[column_name]):\n",
    "        df[column_name] = pd.to_datetime(df[column_name])\n",
    "        \n",
    "def remove_outliers(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes outliers from a dataframe based on the Interquartile Range (IQR) for a given column.\n",
    "    \"\"\"\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Return only rows where the value is within the bounds\n",
    "    return df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
    "\n",
    "def calculate_harmonic_mean(series1, series2):\n",
    "    # Drop NaN values\n",
    "    combined = pd.concat([series1, series2], axis=1).dropna()\n",
    "    harmonic_values = hmean(combined, axis=1)\n",
    "    return pd.Series(harmonic_values, index=combined.index)\n",
    "\n",
    "\n",
    "# Function to iterate over timeseries or use a constant value\n",
    "def iterate_or_constant(value):\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return value\n",
    "    else:\n",
    "        return itertools.repeat(value)\n",
    "\n",
    "# Function using GSW with TEOS-10 conversions\n",
    "def velocity_timeseries_TEOS10(T_series, P_series_kPa, S_series):\n",
    "    velocity_series = []\n",
    "    for T, P_kPa, S in zip(iterate_or_constant(T_series), iterate_or_constant(P_series_kPa), iterate_or_constant(S_series)):\n",
    "        P_dbar = P_kPa * 0.1  # Convert kPa to dbar\n",
    "        SA = gsw.SA_from_SP(S, P_dbar, longitude, latitude)\n",
    "        CT = gsw.CT_from_t(SA, T, P_dbar)\n",
    "        velocity_series.append(gsw.sound_speed(SA, CT, P_dbar))\n",
    "    return velocity_series\n",
    "\n",
    "# Function using basic T, P, S with GSW\n",
    "def velocity_timeseries_basic(T_series, P_series_kPa, S_series):\n",
    "    velocity_series = []\n",
    "    for T, P_kPa, S in zip(iterate_or_constant(T_series), iterate_or_constant(P_series_kPa), iterate_or_constant(S_series)):\n",
    "        P_dbar = P_kPa * 0.1  # Convert kPa to dbar\n",
    "        velocity_series.append(gsw.sound_speed(S, T, P_dbar))\n",
    "    return velocity_series\n",
    "\n",
    "# Function using Chen and Millero equation\n",
    "def velocity_timeseries_chen_millero(T_series, P_series_kPa, S_series):\n",
    "    velocity_series = []\n",
    "    for T, P_kPa, S in zip(iterate_or_constant(T_series), iterate_or_constant(P_series_kPa), iterate_or_constant(S_series)):\n",
    "        P_bar = P_kPa * 0.01  # Convert kPa to bar\n",
    "        velocity_series.append(sound_speed_chen_millero(S, T, P_bar))\n",
    "    return velocity_series\n",
    "\n",
    "# Function to perform linear regression and extrapolation\n",
    "def fit_and_extrapolate(df, start_time, end_time, target_time):\n",
    "    subset = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "    slope, intercept, _, _, _ = linregress(\n",
    "        x=subset['time'].map(pd.Timestamp.timestamp),\n",
    "        y=subset['sea_water_practical_salinity']\n",
    "    )\n",
    "    return slope * target_time.timestamp() + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filepaths to process\n",
    "filepaths = [\n",
    "    \"Data_230912111909_East_006870_2502_.csv\",\n",
    "    \"Data_230912112033_West_006874_2503_.csv\",\n",
    "    \"Data_230913091309_North_00687A_2504_.csv\"\n",
    "]\n",
    "\n",
    "# Process the files again using the updated outlier removal function\n",
    "df_dict = create_nested_dictionary(filepaths)\n",
    "\n",
    "# Convert the 'SoundSpeed (m/s)' column to a numeric data type and then proceed with the outlier removal and dataframe creation using IQR\n",
    "result_dfs = {}\n",
    "identifiers = ['2502', '2503', '2504']\n",
    "\n",
    "for identifier in identifiers:\n",
    "    ssp_df = df_dict[identifier]['SSP'].copy()\n",
    "    ssp_df['SoundSpeed (m/s)'] = pd.to_numeric(ssp_df['SoundSpeed (m/s)'], errors='coerce')\n",
    "    cleaned_df = remove_outliers(ssp_df, 'SoundSpeed (m/s)')\n",
    "    result_df = cleaned_df[['Record Time', 'SoundSpeed (m/s)']]\n",
    "    result_df = result_df.rename(columns={'SoundSpeed (m/s)': 'SoundSpeed'})\n",
    "    result_dfs[identifier] = result_df\n",
    "\n",
    "# Converting the 'Record Time' column in the dataframes within result_dfs to datetime format\n",
    "for identifier, df in result_dfs.items():\n",
    "    df['Record Time'] = pd.to_datetime(df['Record Time'])\n",
    "\n",
    "# Setting the 'SoundSpeed' values of 2503 to NaN\n",
    "result_dfs['2503']['SoundSpeed'] = np.nan\n",
    "\n",
    "\n",
    "# Set larger font sizes\n",
    "plt.rcParams.update({'font.size': 14, 'legend.fontsize': 14})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "identifiers = ['2502', '2504']\n",
    "titles = [\"Filtered SSP for 2502\", \"Filtered SSP for 2504\"]\n",
    "colors = ['blue', 'orange']  # Adjusted color list to match the number of identifiers\n",
    "\n",
    "for identifier, title, color in zip(identifiers, titles, colors):\n",
    "    plt.plot(result_dfs[identifier]['Record Time'], result_dfs[identifier]['SoundSpeed'], \n",
    "             label=title, color=color, linewidth=2)  # Increased line width\n",
    "\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"SoundSpeed (m/s)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)  # Lighter grid lines\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best')  # Optimal legend placement\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of identifiers\n",
    "identifiers = [\"2504\", \"2503\", \"2502\"]\n",
    "\n",
    "# Extracting the data for each identifier\n",
    "data_extracted = {}\n",
    "for identifier in identifiers:\n",
    "    data_extracted[identifier] = {\n",
    "        \"TMP\": df_dict[identifier][\"TMP\"] if \"TMP\" in df_dict[identifier] else None,\n",
    "        \"DQZ\": df_dict[identifier][\"DQZ\"] if \"DQZ\" in df_dict[identifier] else None,\n",
    "        \"INC\": df_dict[identifier][\"INC\"] if \"INC\" in df_dict[identifier] else None\n",
    "    }\n",
    "\n",
    "# Converting columns to numeric type and removing NaN values\n",
    "for identifier in identifiers:\n",
    "    if data_extracted[identifier][\"TMP\"] is not None:\n",
    "        data_extracted[identifier][\"TMP\"][\"Temperature Deg C\"] = pd.to_numeric(data_extracted[identifier][\"TMP\"][\"Temperature Deg C\"], errors='coerce')\n",
    "        data_extracted[identifier][\"TMP\"].dropna(subset=[\"Temperature Deg C\"], inplace=True)\n",
    "    \n",
    "    if data_extracted[identifier][\"DQZ\"] is not None:\n",
    "        data_extracted[identifier][\"DQZ\"][\"Pressure (kPa)\"] = pd.to_numeric(data_extracted[identifier][\"DQZ\"][\"Pressure (kPa)\"], errors='coerce')\n",
    "        data_extracted[identifier][\"DQZ\"].dropna(subset=[\"Pressure (kPa)\"], inplace=True)\n",
    "        \n",
    "    if data_extracted[identifier][\"DQZ\"] is not None:\n",
    "        data_extracted[identifier][\"DQZ\"]['Temperature (Deg C)'] = pd.to_numeric(data_extracted[identifier][\"DQZ\"]['Temperature (Deg C)'], errors='coerce')\n",
    "        data_extracted[identifier][\"DQZ\"].dropna(subset=['Temperature (Deg C)'], inplace=True)\n",
    "        \n",
    "    if data_extracted[identifier][\"INC\"] is not None:\n",
    "        data_extracted[identifier][\"INC\"]['Pitch (deg)'] = pd.to_numeric(data_extracted[identifier][\"INC\"]['Pitch (deg)'], errors='coerce')\n",
    "        data_extracted[identifier][\"INC\"].dropna(subset=['Pitch (deg)'], inplace=True)  \n",
    "        \n",
    "    if data_extracted[identifier][\"INC\"] is not None:\n",
    "        data_extracted[identifier][\"INC\"]['Roll (deg)'] = pd.to_numeric(data_extracted[identifier][\"INC\"]['Roll (deg)'], errors='coerce')\n",
    "        data_extracted[identifier][\"INC\"].dropna(subset=['Roll (deg)'], inplace=True) \n",
    "\n",
    "# Plotting the data after removing outliers\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15),dpi=300)\n",
    "fig.tight_layout(pad=6.0)\n",
    "\n",
    "for idx, identifier in enumerate(identifiers):\n",
    "    # Removing outliers and plotting temperature\n",
    "    if data_extracted[identifier][\"TMP\"] is not None:\n",
    "        data_extracted[identifier][\"TMP\"] = remove_outliers(data_extracted[identifier][\"TMP\"], \"Temperature Deg C\")\n",
    "        axes[idx, 0].plot(pd.to_datetime(data_extracted[identifier][\"TMP\"][\"Record Time\"]), \n",
    "                          data_extracted[identifier][\"TMP\"][\"Temperature Deg C\"])\n",
    "        axes[idx, 0].set_title(f\"Temperature Timeseries for {identifier}\")\n",
    "        axes[idx, 0].set_xlabel(\"Time\")\n",
    "        axes[idx, 0].set_ylabel(\"Temperature (°C)\")\n",
    "    \n",
    "    # Removing outliers and plotting pressure\n",
    "    if data_extracted[identifier][\"DQZ\"] is not None:\n",
    "        data_extracted[identifier][\"DQZ\"] = remove_outliers(data_extracted[identifier][\"DQZ\"], \"Pressure (kPa)\")\n",
    "        data_extracted[identifier][\"DQZ\"] = remove_outliers(data_extracted[identifier][\"DQZ\"], 'Temperature (Deg C)')\n",
    "        axes[idx, 1].plot(pd.to_datetime(data_extracted[identifier][\"DQZ\"][\"Record Time\"]), \n",
    "                          data_extracted[identifier][\"DQZ\"][\"Pressure (kPa)\"])\n",
    "        axes[idx, 1].set_title(f\"Pressure Timeseries for {identifier}\")\n",
    "        axes[idx, 1].set_xlabel(\"Time\")\n",
    "        axes[idx, 1].set_ylabel(\"Pressure (kPa)\")\n",
    "    \n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dfs['2502']['Record Time'][result_dfs['2502']['Record Time'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the dataframes with harmonic means\n",
    "harmonic_mean_dfs = {}\n",
    "\n",
    "# Calculate harmonic means between the time series stored in 2502, 2503, and 2504\n",
    "pairs = [('2502', '2503'), ('2502', '2504'), ('2503', '2504')]\n",
    "\n",
    "# Calculate harmonic means between the time series stored in 2502, 2503, and 2504\n",
    "for pair in pairs:\n",
    "    df1 = result_dfs[pair[0]].set_index('Record Time')\n",
    "    df2 = result_dfs[pair[1]].set_index('Record Time')\n",
    "    df1 = df1[~df1.index.duplicated(keep='first')]\n",
    "    df2 = df2[~df2.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Calculate harmonic mean for each pair and create a new dataframe\n",
    "    harmonic_mean = calculate_harmonic_mean(df1['SoundSpeed'], df2['SoundSpeed'])\n",
    "    harmonic_mean_df = pd.DataFrame({\n",
    "        'Record Time': harmonic_mean.index,\n",
    "        'Harmonic Mean': harmonic_mean.values\n",
    "    })\n",
    "    \n",
    "    harmonic_mean_dfs[f'{pair[0]}_{pair[1]}'] = harmonic_mean_df\n",
    "    \n",
    "# Updating the dataframes based on the instructions\n",
    "harmonic_mean_dfs['2502_2503'] = result_dfs['2502'][['Record Time', 'SoundSpeed']]\n",
    "harmonic_mean_dfs['2503_2504'] = result_dfs['2504'][['Record Time', 'SoundSpeed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_mean_dfs['2502_2504']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salinity Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "file_path = 'ooi-rs03ccal-mj03f-12-ctdpfb305_fedb_e1a6_bd2b.csv'#'ooi-rs03ccal-mj03f-12-ctdpfb305_40d5_df03_3834 (1).csv'\n",
    "df = pd.read_csv(file_path, usecols=[0, 1])\n",
    "\n",
    "# Drop the first row (with units) and reset the index\n",
    "df = df.drop(0).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'time' column to datetime format\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Filter out rows where 'sea_water_practical_salinity' is above 100 or below 34.48\n",
    "df_filtered = df[(df['sea_water_practical_salinity'] <= 100) & (df['sea_water_practical_salinity'] >= 34.48)].copy()\n",
    "\n",
    "# Remove timezone information from 'time'\n",
    "df_filtered.loc[:, 'time'] = df_filtered['time'].dt.tz_localize(None)\n",
    "\n",
    "smoothing_start_date = pd.to_datetime('2023-08-01')\n",
    "\n",
    "# Find the index where smoothing should start\n",
    "smoothing_start_index = df_filtered.index[df_filtered['time'] >= smoothing_start_date][0]\n",
    "\n",
    "# Apply smoothing only to the portion after the start date\n",
    "smoothed_salinity = uniform_filter1d(df_filtered['sea_water_practical_salinity'].iloc[smoothing_start_index:], size=250)\n",
    "\n",
    "# Combine unchanged data with the smoothed portion\n",
    "df_filtered['sea_water_practical_salinity'] = pd.concat([\n",
    "    df_filtered['sea_water_practical_salinity'].iloc[:smoothing_start_index],\n",
    "    pd.Series(smoothed_salinity, index=df_filtered.index[smoothing_start_index:])\n",
    "])\n",
    "\n",
    "# Define the range\n",
    "start_time = pd.to_datetime('2022-08-14 05:49:53')\n",
    "end_time = pd.to_datetime('2024-09-05 23:59:00')\n",
    "\n",
    "# Filter the salinity data based on the defined range\n",
    "mask = (df_filtered['time'] >= start_time) & (df_filtered['time'] <= end_time)\n",
    "salinity_df = df_filtered.loc[mask].copy()\n",
    "\n",
    "\n",
    "# Define the discrete points and their actual salinity values\n",
    "discrete_points = pd.to_datetime(['2022-08-14 05:49:53', '2022-08-30 19:37:13', '2023-09-17 03:35:09'])\n",
    "actual_salinity_values = [34.52543602, 34.5272034, 34.527000]\n",
    "\n",
    "# Function to find the closest time in the DataFrame to a given discrete point\n",
    "def find_closest_time(df, target_time):\n",
    "    absolute_difference = abs(df['time'] - target_time)\n",
    "    closest_index = absolute_difference.idxmin()\n",
    "    return df.loc[closest_index]\n",
    "\n",
    "# First Correction: Adjust the entire series to align with the second discrete point\n",
    "second_point_data = find_closest_time(salinity_df, discrete_points[1])\n",
    "second_point_salinity = second_point_data['sea_water_practical_salinity']\n",
    "salinity_adjustment = actual_salinity_values[1] - second_point_salinity\n",
    "salinity_df.loc[:, 'adjusted_salinity'] = salinity_df['sea_water_practical_salinity'] + salinity_adjustment\n",
    "\n",
    "# Extrapolation function using linregress\n",
    "def fit_and_extrapolate(df, start_time, end_time, target_time):\n",
    "    subset = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
    "    slope, intercept, _, _, _ = linregress(\n",
    "        x=subset['time'].map(pd.Timestamp.timestamp),\n",
    "        y=subset['adjusted_salinity']\n",
    "    )\n",
    "    return slope * target_time.timestamp() + intercept\n",
    "\n",
    "# Third Correction: Drift correction for data between second and third discrete points\n",
    "fit_start_2 = pd.to_datetime('2023-06-15 03:35:09')\n",
    "fit_end_2 = pd.to_datetime('2024-09-07 23:59:00')\n",
    "extrapolated_end_salinity = fit_and_extrapolate(salinity_df, fit_start_2, fit_end_2, discrete_points[2])\n",
    "drift_correction_at_third_point = actual_salinity_values[2] - extrapolated_end_salinity\n",
    "\n",
    "# Apply linear correction between the second and third discrete points\n",
    "time_difference = discrete_points[2] - discrete_points[1]\n",
    "salinity_df.loc[:, 'second_stage_corrected_salinity'] = salinity_df.apply(\n",
    "    lambda row: row['adjusted_salinity'] + drift_correction_at_third_point * (1 - (discrete_points[2] - row['time']) / time_difference) if discrete_points[1] <= row['time'] <= discrete_points[2] else row['adjusted_salinity'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# New Correction: Adjust salinity values between first and second discrete points\n",
    "fit_start_1 = pd.to_datetime('2022-08-14 15:54:00')\n",
    "fit_end_1 = pd.to_datetime('2022-09-15 03:35:09')\n",
    "extrapolated_first_point_salinity = fit_and_extrapolate(salinity_df, fit_start_1, fit_end_1, discrete_points[0])\n",
    "drift_correction_at_first_point = actual_salinity_values[0] - extrapolated_first_point_salinity\n",
    "\n",
    "# Apply linear correction between the first and second discrete points\n",
    "time_difference = discrete_points[1] - discrete_points[0]\n",
    "salinity_df.loc[:, 'corrected_salinity'] = salinity_df.apply(\n",
    "    lambda row: row['second_stage_corrected_salinity'] + drift_correction_at_first_point * (1 - (row['time'] - discrete_points[0]) / time_difference) if discrete_points[0] <= row['time'] <= discrete_points[1] else row['second_stage_corrected_salinity'],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "#plt.plot(salinity_df['time'], salinity_df['sea_water_practical_salinity'], label=\"Original Salinity\", alpha=0.7)\n",
    "plt.plot(salinity_df['time'], salinity_df['corrected_salinity'], label=\"After Final Correction\", linestyle=':', color = 'black')\n",
    "#plt.scatter(discrete_points, actual_salinity_values, color='red', marker='o', label=\"Discrete Data Points\")\n",
    "#plt.plot(discrete_points[0:3], actual_salinity_values[0:3],'-.', label=\"Discrete Points\", color='red')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Salinity (PSU)')\n",
    "#plt.title('Salinity Corrections over Time with Discrete Data Points')\n",
    "#plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming salinity_df is your DataFrame with the corrected salinity data\n",
    "# and df_discrete contains the discrete points with actual salinity values\n",
    "\n",
    "# Define the time range to zoom in (1st and 2nd discrete points)\n",
    "start_time = discrete_points[0]- pd.Timedelta(days=20)  # first discrete point\n",
    "end_time = discrete_points[1]+ pd.Timedelta(days=5)    # second discrete point\n",
    "\n",
    "\n",
    "# Filter the DataFrame for the zoomed-in range\n",
    "zoomed_df = salinity_df[(salinity_df['time'] >= start_time) & (salinity_df['time'] <= end_time)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(zoomed_df['time'], zoomed_df['sea_water_practical_salinity'],'--', label=\"Corrected Salinity\", color='blue')\n",
    "plt.plot(zoomed_df['time'], zoomed_df['corrected_salinity'],'--', label=\"2nd\", color='orange')\n",
    "#plt.plot(zoomed_df['time'], zoomed_df['third_stage_corrected_salinity'],'--', label=\"3rd\", color='green')\n",
    "#plt.plot(discrete_points[0:2], actual_salinity_values[0:2],'-.', label=\"Discrete Points\", color='red')\n",
    "\n",
    "plt.scatter(discrete_points[0:2], actual_salinity_values[0:2], label=\"Discrete Points\", color='red')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Salinity (PSU)')\n",
    "plt.title('Salinity Correction between 1st and 2nd Discrete Points')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the 'Record Time' to datetime and set as index for pressure DataFrame\n",
    "pressure_df4 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2504']['DQZ']['Record Time']),\n",
    "    'Pressure (kPa)': data_extracted['2504']['DQZ']['Pressure (kPa)']\n",
    "}).set_index('Record Time')\n",
    "\n",
    "DQZtemp_df4 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2504']['DQZ']['Record Time']),\n",
    "    'Temperature Deg C': data_extracted['2504']['DQZ']['Temperature (Deg C)']\n",
    "}).set_index('Record Time')\n",
    "\n",
    "# Convert the 'Record Time' to datetime and set as index for temperature DataFrame\n",
    "temperature_df4 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2504']['TMP']['Record Time']),\n",
    "    'Temperature Deg C': data_extracted['2504']['TMP']['Temperature Deg C']+0.351\n",
    "}).set_index('Record Time')\n",
    "\n",
    "INC_df4 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2504']['INC']['Record Time']),\n",
    "    'Pitch': data_extracted['2504']['INC']['Pitch (deg)'],\n",
    "    'Roll': data_extracted['2504']['INC']['Roll (deg)']\n",
    "})\n",
    "\n",
    "# Merge pressure and temperature data on their indexes (Record Time)\n",
    "combined_df4 = pressure_df4.join(temperature_df4, how='left')\n",
    "DQZtemp_df4 = DQZtemp_df4[~DQZtemp_df4.index.duplicated(keep='first')]\n",
    "combined_df4['TempDQZ']=DQZtemp_df4['Temperature Deg C']\n",
    "\n",
    "# Now, for salinity, which has different times, let's create a DataFrame and reindex it to the pressure times,\n",
    "# filling missing values by interpolating or forward-filling (depending on your needs)\n",
    "salinity_dfs4 = pd.DataFrame({\n",
    "    'time': pd.to_datetime(salinity_df['time']),\n",
    "    'adjusted_salinity': salinity_df['corrected_salinity']\n",
    "}).set_index('time')\n",
    "\n",
    "# Reindex the salinity data to match the pressure record times, forward-filling values\n",
    "aligned_salinity_df4 = salinity_dfs4.reindex(combined_df4.index, method='nearest')\n",
    "\n",
    "# Add the aligned salinity data to the combined dataframe\n",
    "combined_df4['Salinity'] = aligned_salinity_df4['adjusted_salinity']\n",
    "\n",
    "# Reset index to make 'Record Time' a column again, if needed\n",
    "combined_df4.reset_index(inplace=True)\n",
    "\n",
    "# Rename the index to 'Record Time'\n",
    "combined_df4.rename(columns={'index': 'Record Time'}, inplace=True)\n",
    "\n",
    "combined_df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Record Time' to datetime and set as index for pressure DataFrame\n",
    "pressure_df3 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2503']['DQZ']['Record Time']),\n",
    "    'Pressure (kPa)': data_extracted['2503']['DQZ']['Pressure (kPa)']\n",
    "}).set_index('Record Time')\n",
    "\n",
    "# Convert the 'Record Time' to datetime and set as index for temperature DataFrame\n",
    "temperature_df3 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2503']['TMP']['Record Time']),\n",
    "    'Temperature Deg C': data_extracted['2503']['TMP']['Temperature Deg C']\n",
    "}).set_index('Record Time')\n",
    "\n",
    "INC_df3 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2503']['INC']['Record Time']),\n",
    "    'Pitch': data_extracted['2503']['INC']['Pitch (deg)'],\n",
    "    'Roll': data_extracted['2503']['INC']['Roll (deg)']\n",
    "})\n",
    "\n",
    "# Merge pressure and temperature data on their indexes (Record Time)\n",
    "combined_df3 = pressure_df3.join(temperature_df3, how='left')\n",
    "\n",
    "# Now, for salinity, which has different times, let's create a DataFrame and reindex it to the pressure times,\n",
    "# filling missing values by interpolating or forward-filling (depending on your needs)\n",
    "salinity_dfs3 = pd.DataFrame({\n",
    "    'time': pd.to_datetime(salinity_df['time']),\n",
    "    'adjusted_salinity': salinity_df['corrected_salinity']\n",
    "}).set_index('time')\n",
    "\n",
    "# Reindex the salinity data to match the pressure record times, forward-filling values\n",
    "aligned_salinity_df3 = salinity_dfs3.reindex(combined_df3.index, method='nearest')\n",
    "\n",
    "# Add the aligned salinity data to the combined dataframe\n",
    "combined_df3['Salinity'] = aligned_salinity_df3['adjusted_salinity']\n",
    "\n",
    "# Reset index to make 'Record Time' a column again, if needed\n",
    "combined_df3.reset_index(inplace=True)\n",
    "\n",
    "# Rename the index to 'Record Time'\n",
    "combined_df3.rename(columns={'index': 'Record Time'}, inplace=True)\n",
    "\n",
    "combined_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Record Time' to datetime and set as index for pressure DataFrame\n",
    "pressure_df = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2502']['DQZ']['Record Time']),\n",
    "    'Pressure (kPa)': data_extracted['2502']['DQZ']['Pressure (kPa)']\n",
    "}).set_index('Record Time')\n",
    "\n",
    "# Convert the 'Record Time' to datetime and set as index for temperature DataFrame\n",
    "temperature_df1 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2502']['TMP']['Record Time']),\n",
    "    'Temperature Deg C': data_extracted['2502']['TMP']['Temperature Deg C']\n",
    "}).set_index('Record Time')\n",
    "\n",
    "INC_df2 = pd.DataFrame({\n",
    "    'Record Time': pd.to_datetime(data_extracted['2502']['INC']['Record Time']),\n",
    "    'Pitch': data_extracted['2502']['INC']['Pitch (deg)'],\n",
    "    'Roll': data_extracted['2502']['INC']['Roll (deg)']\n",
    "})\n",
    "\n",
    "# Merge pressure and temperature data on their indexes (Record Time)\n",
    "combined_df2 = pressure_df.join(temperature_df1, how='left')\n",
    "\n",
    "# Now, for salinity, which has different times, let's create a DataFrame and reindex it to the pressure times,\n",
    "# filling missing values by interpolating or forward-filling (depending on your needs)\n",
    "salinity_dfs2 = pd.DataFrame({\n",
    "    'time': pd.to_datetime(salinity_df['time']),\n",
    "    'adjusted_salinity': salinity_df['corrected_salinity']\n",
    "}).set_index('time')\n",
    "\n",
    "# Reindex the salinity data to match the pressure record times, forward-filling values\n",
    "aligned_salinity_df2 = salinity_dfs2.reindex(combined_df2.index, method='nearest')\n",
    "\n",
    "# Add the aligned salinity data to the combined dataframe\n",
    "combined_df2['Salinity'] = aligned_salinity_df2['adjusted_salinity']\n",
    "\n",
    "# Reset index to make 'Record Time' a column again, if needed\n",
    "combined_df2.reset_index(inplace=True)\n",
    "\n",
    "# Rename the index to 'Record Time'\n",
    "combined_df2.rename(columns={'index': 'Record Time'}, inplace=True)\n",
    "\n",
    "combined_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch and Roll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INC_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INC_dfs3 = INC_df3[(INC_df3['Pitch'] < -3) & (INC_df3['Pitch'] > -3.5)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.scatter(INC_dfs3['Record Time'], INC_dfs3['Pitch'], s=2, label='2503 Pitch')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Pitch (Deg)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INC_dfs3 = INC_df3[(INC_df3['Roll'] < 0)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.scatter(INC_dfs3['Record Time'], INC_dfs3['Roll'], s=2, label='2503 Roll')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Roll (Deg)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pressure Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new correction model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def parse_to_dataframe(data):\n",
    "    lines = data.split('\\n')\n",
    "    parsed_data = [line.split() for line in lines if line]\n",
    "    df = pd.DataFrame(parsed_data, columns=['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', 'Value'])\n",
    "    df['DateTime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']])\n",
    "    df['Value'] = pd.to_numeric(df['Value'])\n",
    "    df.drop(['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Reading the contents of the files\n",
    "with open('pred_F_2022.txt', 'r') as file_2022, open('pred_F_2023.txt', 'r') as file_2023, open('pred_F_2024.txt', 'r') as file_2024:\n",
    "    data_2022 = file_2022.read()\n",
    "    data_2023 = file_2023.read()\n",
    "    data_2024 = file_2024.read()\n",
    "\n",
    "df_2022 = parse_to_dataframe(data_2022)\n",
    "df_2023 = parse_to_dataframe(data_2023)\n",
    "df_2024 = parse_to_dataframe(data_2024)\n",
    "\n",
    "tidal_df = pd.concat([df_2022, df_2023,df_2024], ignore_index=True)\n",
    "tidal_df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Assuming combined_df4 is already defined and contains pressure data\n",
    "#combined_df4['Pressure (kPa)'] and combined_df4['Record Time']\n",
    "\n",
    "# Convert 'Record Time' to DateTime and set as index\n",
    "combined_df4['DateTime'] = pd.to_datetime(combined_df4['Record Time'])\n",
    "combined_df4.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Optimization function\n",
    "def optimize_tidal_influence(params, tidal_df, pressure_df):\n",
    "    amplitude, rho = params\n",
    "    adjusted_tidal = amplitude * tidal_df['Value']\n",
    "    tidal_df['Adjusted Value'] = adjusted_tidal\n",
    "    interpolated_tidal = tidal_df.reindex(pressure_df.index, method='nearest')['Adjusted Value']\n",
    "    corrected_pressure = pressure_df['Pressure (kPa)'] - (rho * 9.81 * interpolated_tidal) * 0.001\n",
    "    return np.var(corrected_pressure)\n",
    "\n",
    "# Initial guesses for amplitude, phase, and rho\n",
    "initial_amplitude = 1  # Adjust based on your data\n",
    "initial_phase = 0      # Adjust based on your data\n",
    "initial_rho = 1025     # Initial guess for the density of seawater (kg/m^3)\n",
    "\n",
    "# Perform optimization\n",
    "result = least_squares(optimize_tidal_influence, x0=[initial_amplitude, initial_rho],\n",
    "                       args=(tidal_df, combined_df4))\n",
    "\n",
    "optimized_amplitude, optimized_rho = result.x\n",
    "\n",
    "# Apply optimized parameters\n",
    "tidal_df['Adjusted Value'] = optimized_amplitude * tidal_df['Value']\n",
    "combined_df4['Tidal Influence (kPa)'] = (optimized_rho * 9.81 * tidal_df.reindex(combined_df4.index, method='nearest')['Adjusted Value']) * 0.001\n",
    "combined_df4['Corrected Pressure (kPa)'] = combined_df4['Pressure (kPa)'] - combined_df4['Tidal Influence (kPa)']\n",
    "\n",
    "# Proceed with any further analysis or steps as needed\n",
    "# Reset the index to make 'Record Time' a column again\n",
    "combined_df4.reset_index(inplace=True)\n",
    "\n",
    "del tidal_df\n",
    "combined_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new correction model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def parse_to_dataframe(data):\n",
    "    lines = data.split('\\n')\n",
    "    parsed_data = [line.split() for line in lines if line]\n",
    "    df = pd.DataFrame(parsed_data, columns=['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', 'Value'])\n",
    "    df['DateTime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']])\n",
    "    df['Value'] = pd.to_numeric(df['Value'])\n",
    "    df.drop(['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Reading the contents of the files\n",
    "with open('pred_F_2022.txt', 'r') as file_2022, open('pred_F_2023.txt', 'r') as file_2023:\n",
    "    data_2022 = file_2022.read()\n",
    "    data_2023 = file_2023.read()\n",
    "\n",
    "df_2022 = parse_to_dataframe(data_2022)\n",
    "df_2023 = parse_to_dataframe(data_2023)\n",
    "\n",
    "tidal_df = pd.concat([df_2022, df_2023], ignore_index=True)\n",
    "tidal_df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Assuming combined_df4 is already defined and contains pressure data\n",
    "# combined_df4['Pressure (kPa)'] and combined_df4['Record Time']\n",
    "\n",
    "# Convert 'Record Time' to DateTime and set as index\n",
    "combined_df3['DateTime'] = pd.to_datetime(combined_df3['Record Time'])\n",
    "combined_df3.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Optimization function\n",
    "def optimize_tidal_influence(params, tidal_df, pressure_df):\n",
    "    amplitude, rho = params\n",
    "    adjusted_tidal = amplitude * tidal_df['Value']\n",
    "    tidal_df['Adjusted Value3'] = adjusted_tidal\n",
    "    interpolated_tidal = tidal_df.reindex(pressure_df.index, method='nearest')['Adjusted Value3']\n",
    "    corrected_pressure = pressure_df['Pressure (kPa)'] - (rho * 9.81 * interpolated_tidal) * 0.001\n",
    "    return np.var(corrected_pressure)\n",
    "\n",
    "# Initial guesses for amplitude, phase, and rho\n",
    "initial_amplitude = 1  # Adjust based on your data\n",
    "initial_phase = 0      # Adjust based on your data\n",
    "initial_rho = 1025     # Initial guess for the density of seawater (kg/m^3)\n",
    "\n",
    "# Perform optimization\n",
    "result = least_squares(optimize_tidal_influence, x0=[initial_amplitude, initial_rho],\n",
    "                       args=(tidal_df, combined_df3))\n",
    "\n",
    "optimized_amplitude, optimized_rho = result.x\n",
    "\n",
    "# Apply optimized parameters\n",
    "tidal_df['Adjusted Value3'] = optimized_amplitude * tidal_df['Value']\n",
    "combined_df3['Tidal Influence (kPa)'] = (optimized_rho * 9.81 * tidal_df.reindex(combined_df3.index, method='nearest')['Adjusted Value3']) * 0.001\n",
    "combined_df3['Corrected Pressure (kPa)'] = combined_df3['Pressure (kPa)'] - combined_df3['Tidal Influence (kPa)']\n",
    "\n",
    "# Proceed with any further analysis or steps as needed\n",
    "# Reset the index to make 'Record Time' a column again\n",
    "combined_df3.reset_index(inplace=True)\n",
    "combined_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new correction model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def parse_to_dataframe(data):\n",
    "    lines = data.split('\\n')\n",
    "    parsed_data = [line.split() for line in lines if line]\n",
    "    df = pd.DataFrame(parsed_data, columns=['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', 'Value'])\n",
    "    df['DateTime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']])\n",
    "    df['Value'] = pd.to_numeric(df['Value'])\n",
    "    df.drop(['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Reading the contents of the files\n",
    "with open('pred_F_2022.txt', 'r') as file_2022, open('pred_F_2023.txt', 'r') as file_2023:\n",
    "    data_2022 = file_2022.read()\n",
    "    data_2023 = file_2023.read()\n",
    "\n",
    "df_2022 = parse_to_dataframe(data_2022)\n",
    "df_2023 = parse_to_dataframe(data_2023)\n",
    "\n",
    "tidal_df = pd.concat([df_2022, df_2023], ignore_index=True)\n",
    "tidal_df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Assuming combined_df4 is already defined and contains pressure data\n",
    "# combined_df4['Pressure (kPa)'] and combined_df4['Record Time']\n",
    "\n",
    "# Convert 'Record Time' to DateTime and set as index\n",
    "combined_df2['DateTime'] = pd.to_datetime(combined_df2['Record Time'])\n",
    "combined_df2.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Optimization function\n",
    "def optimize_tidal_influence(params, tidal_df, pressure_df):\n",
    "    amplitude, rho = params\n",
    "    adjusted_tidal = amplitude * tidal_df['Value']\n",
    "    tidal_df['Adjusted Value2'] = adjusted_tidal\n",
    "    interpolated_tidal = tidal_df.reindex(pressure_df.index, method='nearest')['Adjusted Value2']\n",
    "    corrected_pressure = pressure_df['Pressure (kPa)'] - (rho * 9.81 * interpolated_tidal) * 0.001\n",
    "    return np.var(corrected_pressure)\n",
    "\n",
    "# Initial guesses for amplitude, phase, and rho\n",
    "initial_amplitude = 1  # Adjust based on your data\n",
    "initial_phase = 0      # Adjust based on your data\n",
    "initial_rho = 1025     # Initial guess for the density of seawater (kg/m^3)\n",
    "\n",
    "# Perform optimization\n",
    "result = least_squares(optimize_tidal_influence, x0=[initial_amplitude, initial_rho],\n",
    "                       args=(tidal_df, combined_df2))\n",
    "\n",
    "optimized_amplitude, optimized_rho = result.x\n",
    "\n",
    "# Apply optimized parameters\n",
    "tidal_df['Adjusted Value2'] = optimized_amplitude * tidal_df['Value']\n",
    "combined_df2['Tidal Influence (kPa)'] = (optimized_rho * 9.81 * tidal_df.reindex(combined_df2.index, method='nearest')['Adjusted Value2']) * 0.001\n",
    "combined_df2['Corrected Pressure (kPa)'] = combined_df2['Pressure (kPa)'] - combined_df2['Tidal Influence (kPa)']\n",
    "\n",
    "# Proceed with any further analysis or steps as needed\n",
    "# Reset the index to make 'Record Time' a column again\n",
    "combined_df2.reset_index(inplace=True)\n",
    "combined_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "# Create a function to interpolate velocity\n",
    "def interpolate_velocity(time_series, velocity_df):\n",
    "    # Convert 'Record Time' to numeric for interpolation\n",
    "    velocity_df['Record Time'] = pd.to_datetime(velocity_df['Record Time'])\n",
    "    time_numeric = velocity_df['Record Time'].astype('int64')\n",
    "    \n",
    "    # Interpolator function\n",
    "    interpolator = interp1d(time_numeric, velocity_df['SoundSpeed'], \n",
    "                            fill_value='extrapolate', bounds_error=False)\n",
    "    \n",
    "    # Apply interpolation\n",
    "    return interpolator(pd.to_datetime(time_series).astype('int64'))\n",
    "\n",
    "# Apply interpolation to get velocity values for combined_df4 datetimes\n",
    "combined_df4['Record Time'] = pd.to_datetime(combined_df4['Record Time'])\n",
    "combined_df4['interp_v'] = interpolate_velocity(combined_df4['Record Time'], result_dfs['2504'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "#plt.plot(salinity_df['time'], salinity_df['sea_water_practical_salinity'], label=\"Original Salinity\", alpha=0.7)\n",
    "plt.plot(salinity_df['time'], salinity_df['corrected_salinity'], label=\"After Final Correction\", linestyle=':', color = 'black')\n",
    "#plt.scatter(discrete_points, actual_salinity_values, color='red', marker='o', label=\"Discrete Data Points\")\n",
    "#plt.plot(discrete_points[0:3], actual_salinity_values[0:3],'-.', label=\"Discrete Points\", color='red')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Salinity (PSU)')\n",
    "#plt.title('Salinity Corrections over Time with Discrete Data Points')\n",
    "#plt.legend()\n",
    "#plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(combined_df2['Record Time'], combined_df2['Pressure (kPa)'], label='2504 Measured Pressure')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.ylabel(\"Pressure (kPa)\")\n",
    "plt.xlabel('Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(combined_df2['Record Time'],combined_df2['Temperature Deg C'],label='Temperature', color = 'orange')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature Deg C')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(result_dfs['2502']['Record Time'], result_dfs['2502']['SoundSpeed'],label='2502 Recorded Velocity', color = 'green')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Velocity m/s')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 4 subplots, each having a size of 16x4\n",
    "fig, axs = plt.subplots(4, 1, figsize=(16, 13))\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "# Plot 1: Velocity\n",
    "axs[0].plot(result_dfs['2502']['Record Time'], result_dfs['2502']['SoundSpeed'], label='2502 Recorded Velocity', color='green')\n",
    "axs[0].plot(combined_df2['Record Time'],combined_df2['velocity'], label='2502 Calculated Velocity', color='black',linewidth=1)\n",
    "axs[0].set_ylabel('Velocity m/s')\n",
    "axs[0].tick_params(axis='x', labelbottom=False)  # Remove x-axis labels\n",
    "\n",
    "# Plot 2: Temperature\n",
    "axs[1].plot(combined_df2['Record Time'], combined_df2['Temperature Deg C'], label='Temperature', color='orange')\n",
    "axs[1].set_ylabel('Temperature C°')\n",
    "axs[1].tick_params(axis='x', labelbottom=False)  # Remove x-axis labels\n",
    "\n",
    "# Plot 3: Pressure\n",
    "axs[2].plot(combined_df2['Record Time'], combined_df2['Pressure (kPa)'], label='2504 Measured Pressure')\n",
    "axs[2].set_ylabel('Pressure (kPa)')\n",
    "axs[2].tick_params(axis='x', labelbottom=False)  # Remove x-axis labels\n",
    "\n",
    "# Plot 4: Salinity\n",
    "axs[3].plot(salinity_df['time'], salinity_df['corrected_salinity'], label=\"After Final Correction\", linestyle=':', color='black')\n",
    "axs[3].set_xlabel('Time')\n",
    "axs[3].set_ylabel('Salinity (PSU)')\n",
    "axs[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 12))  # 3 subplots, one above the other\n",
    "\n",
    "# Adjust the font size\n",
    "plt.rcParams.update({'font.size': 14, 'legend.fontsize': 14})\n",
    "\n",
    "# First subplot for combined_df2\n",
    "axs[0].plot(combined_df2['Record Time'][200:300], combined_df2['Pressure (kPa)'][200:300], color='gold', label='2502 Measured Pressure')\n",
    "axs[0].plot(combined_df2['Record Time'][200:300], combined_df2['Corrected Pressure (kPa)'][200:300], color='red', label='Measured Pressure - Tidal Model')\n",
    "axs[0].set_ylabel(\"Pressure (kPa)\")\n",
    "axs[0].legend()\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Second subplot for combined_df3\n",
    "axs[1].plot(combined_df3['Record Time'][200:300], combined_df3['Pressure (kPa)'][200:300], color='green', label='2503 Measured Pressure')\n",
    "axs[1].plot(combined_df3['Record Time'][200:300], combined_df3['Corrected Pressure (kPa)'][200:300], color='red', label='Measured Pressure - Tidal Model')\n",
    "axs[1].set_ylabel(\"Pressure (kPa)\")\n",
    "axs[1].legend()\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Third subplot for combined_df4\n",
    "axs[2].plot(combined_df4['Record Time'][200:300], combined_df4['Pressure (kPa)'][200:300], label='2504 Measured Pressure')\n",
    "axs[2].plot(combined_df4['Record Time'][200:300], combined_df4['Corrected Pressure (kPa)'][200:300], color='red', label='Measured Pressure - Tidal Model')\n",
    "axs[2].set_ylabel(\"Pressure (kPa)\")\n",
    "axs[2].legend()\n",
    "axs[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axs[3].plot(combined_df3['Record Time'][200:300], combined_df3['Tidal Influence (kPa)'][200:300], color='black', label='Tidal Model Pressure')\n",
    "axs[3].set_xlabel(\"Time\")\n",
    "axs[3].set_ylabel(\"Pressure (kPa)\")\n",
    "axs[3].legend()\n",
    "axs[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gsw\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming combined_df4 and result_dfs are already loaded DataFrames\n",
    "\n",
    "\n",
    "\n",
    "def velocity_from_teos10(temp, salinity, pressure):\n",
    "    # Convert pressure from kPa to dbar (1 dbar = 10 kPa)\n",
    "    pressure_dbar = pressure*0.1\n",
    "    # Latitude and Longitude\n",
    "    longitude = -130.01149966\n",
    "    latitude = 45.95882833\n",
    "\n",
    "    # Calculate Absolute Salinity (SA) and Conservative Temperature (CT)\n",
    "    SA = gsw.SA_from_SP(salinity, pressure_dbar, longitude, latitude)\n",
    "    CT = gsw.CT_from_t(SA, temp, pressure_dbar)\n",
    "\n",
    "    # Using gsw package for TEOS-10 sound speed calculation\n",
    "    return gsw.sound_speed(CT, SA, pressure_dbar)\n",
    "\n",
    "def objective_function(delta_T, temp_obs, salinity, pressure, velocity_obs):\n",
    "    temp_true = temp_obs + delta_T\n",
    "    velocity_calc = velocity_from_teos10(temp_true, salinity, pressure)\n",
    "    mse = np.mean((velocity_calc - velocity_obs) ** 2)\n",
    "    return mse\n",
    "\n",
    "def find_best_temperature_offset(temp_obs, salinity, pressure, velocity_obs):\n",
    "    result = minimize(objective_function, x0=0, args=(temp_obs, salinity, pressure, velocity_obs))\n",
    "    return result.x\n",
    "\n",
    "# Extracting data for analysis\n",
    "temp_obs = combined_df4['Temperature Deg C'].values\n",
    "salinity = combined_df4['Salinity'].values\n",
    "pressure = combined_df4['Corrected Pressure (kPa)'].values\n",
    "interp_v = combined_df4['interp_v'].values\n",
    "\n",
    "# Finding the best temperature offset\n",
    "best_offset = find_best_temperature_offset(temp_obs, salinity, pressure, interp_v)\n",
    "print(f\"Best-fit Temperature Offset: {best_offset}\")\n",
    "\n",
    "# Calculating velocity with best-fit temperature offset\n",
    "adjusted_temp = temp_obs + best_offset\n",
    "calculated_velocity = velocity_from_teos10(adjusted_temp, salinity, pressure)\n",
    "#calculated_velocity0 = velocity_from_teos10(temp_obs, salinity, pressure)\n",
    "\n",
    "# Plotting the results for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df4['Record Time'], interp_v, label='Interpolated Observed Sound Speed', color='blue')\n",
    "plt.plot(combined_df4['Record Time'], calculated_velocity, label='Calculated Velocity with Adjusted Temperature', color='red', linestyle='--')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.title('Comparison of Interpolated Observed Sound Speed and Calculated Velocity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "temp_obs\n",
    "\n",
    "longitude = -130.01149966\n",
    "latitude = 45.95882833\n",
    "temp_obs = combined_df4['Temperature Deg C'].values\n",
    "adjusted_temp = temp_obs + 0.2\n",
    "calculated_velocity0 = velocity_timeseries_TEOS10(temp_obs, salinity, pressure)\n",
    "calculated_velocity1 = velocity_timeseries_TEOS10(temp_obs+0.1, salinity, pressure)\n",
    "calculated_velocity2 = velocity_timeseries_TEOS10(temp_obs+0.2, salinity, pressure)\n",
    "calculated_velocity3 = velocity_timeseries_TEOS10(temp_obs+0.3, salinity, pressure)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(result_dfs['2504']['Record Time'], result_dfs['2504']['SoundSpeed'], label='2504', color='red')\n",
    "\n",
    "plt.plot(combined_df4['Record Time'], calculated_velocity0, label='Calculated Velocity with Temperature', linestyle='--')\n",
    "plt.plot(combined_df4['Record Time'], calculated_velocity2, label='Calculated Velocity with Adjusted Temperature 0.2', linestyle='--')\n",
    "plt.plot(combined_df4['Record Time'], calculated_velocity3, label='Calculated Velocity with Adjusted Temperature 0.3', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "plt.title('Comparison of Interpolated Observed Sound Speed and Calculated Velocity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "v=velocity_from_teos10(combined_df4['Temperature Deg C'].values+0.4,combined_df4['Salinity'].values,combined_df4['Corrected Pressure (kPa)'].values)\n",
    "v1=velocity_from_teos10(combined_df4['Temperature Deg C'].values,combined_df4['Salinity'].values,combined_df4['Corrected Pressure (kPa)'].values)\n",
    "plt.plot(combined_df4['Record Time'],v1,label='adjusted')\n",
    "plt.plot(combined_df4['Record Time'],v,label='unadjusted')\n",
    "plt.plot(result_dfs['2504']['Record Time'],result_dfs['2504']['SoundSpeed'],label = 'recorded')\n",
    "#plt.plot(combined_df4['Record Time'], interpolated_velocity, label='Interpolated Observed Sound Speed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsw\n",
    "\n",
    "# Initial conditions\n",
    "salinity = 34.5265  # PSU\n",
    "temperature = 1.98  # degrees Celsius\n",
    "pressure_kpa = 15561  # kPa\n",
    "pressure_dbar = pressure_kpa * 0.1  # dbar for GSW\n",
    "pressure_bar_cm = pressure_kpa * 0.01  # bars for Chen and Millero\n",
    "longitude = -130.01149966\n",
    "latitude = 45.95882833\n",
    "\n",
    "# Function to calculate sensitivities and baseline velocity using GSW with TEOS-10 conversions\n",
    "def calculate_sensitivities_TEOS10(salinity, temperature, pressure_dbar, longitude, latitude):\n",
    "    delta = 0.1  # Change for sensitivity calculations\n",
    "    SA_ref = gsw.SA_from_SP(salinity, pressure_dbar, longitude, latitude)\n",
    "    CT_ref = gsw.CT_from_t(SA_ref, temperature, pressure_dbar)\n",
    "    baseline_velocity = gsw.sound_speed(SA_ref, CT_ref, pressure_dbar)\n",
    "\n",
    "    # Sensitivity to salinity\n",
    "    SA_new = gsw.SA_from_SP(salinity + delta, pressure_dbar, longitude, latitude)\n",
    "    sensitivity_S = (gsw.sound_speed(SA_new, CT_ref, pressure_dbar) - baseline_velocity) / delta\n",
    "\n",
    "    # Sensitivity to temperature\n",
    "    CT_new = gsw.CT_from_t(SA_ref, temperature + delta, pressure_dbar)\n",
    "    sensitivity_T = (gsw.sound_speed(SA_ref, CT_new, pressure_dbar) - baseline_velocity) / delta\n",
    "\n",
    "    # Sensitivity to pressure\n",
    "    sensitivity_P = (gsw.sound_speed(SA_ref, CT_ref, pressure_dbar + delta) - baseline_velocity) / delta\n",
    "\n",
    "    return baseline_velocity, sensitivity_S, sensitivity_T, sensitivity_P\n",
    "\n",
    "# Function to calculate sensitivities and baseline velocity using basic T, P, S\n",
    "def calculate_sensitivities_basic(salinity, temperature, pressure_dbar):\n",
    "    delta = 0.1  # Change for sensitivity calculations\n",
    "    baseline_velocity = gsw.sound_speed(salinity, temperature, pressure_dbar)\n",
    "\n",
    "    # Sensitivity to salinity\n",
    "    sensitivity_S = (gsw.sound_speed(salinity + delta, temperature, pressure_dbar) - baseline_velocity) / delta\n",
    "\n",
    "    # Sensitivity to temperature\n",
    "    sensitivity_T = (gsw.sound_speed(salinity, temperature + delta, pressure_dbar) - baseline_velocity) / delta\n",
    "\n",
    "    # Sensitivity to pressure\n",
    "    sensitivity_P = (gsw.sound_speed(salinity, temperature, pressure_dbar + delta) - baseline_velocity) / delta\n",
    "\n",
    "    return baseline_velocity, sensitivity_S, sensitivity_T, sensitivity_P\n",
    "\n",
    "\n",
    "# Perform calculations\n",
    "baseline_velocity_gsw_TEOS10, sensitivity_S_gsw_TEOS10, sensitivity_T_gsw_TEOS10, sensitivity_P_gsw_TEOS10 = calculate_sensitivities_TEOS10(salinity, temperature, pressure_dbar, longitude, latitude)\n",
    "baseline_velocity_gsw_basic, sensitivity_S_gsw_basic, sensitivity_T_gsw_basic, sensitivity_P_gsw_basic = calculate_sensitivities_basic(salinity, temperature, pressure_dbar)\n",
    "\n",
    "# Initial data: Changes in velocity for standard deviation changes in each variable\n",
    "delta_v_temperature = 0.1133  # Change in velocity for 0.025°C change in temperature (m/s)\n",
    "delta_v_pressure = 0.0119     # Change in velocity for 7 kPa change in pressure (m/s)\n",
    "delta_v_salinity = 0.0049     # Change in velocity for 0.003 PSU change in salinity (m/s)\n",
    "\n",
    "# Squaring the changes in velocity\n",
    "delta_v_temperature_squared = delta_v_temperature ** 2\n",
    "delta_v_pressure_squared = delta_v_pressure ** 2\n",
    "delta_v_salinity_squared = delta_v_salinity ** 2\n",
    "\n",
    "# Calculating the total impact of all squared changes\n",
    "total_squared_changes = delta_v_temperature_squared + delta_v_pressure_squared + delta_v_salinity_squared\n",
    "\n",
    "# Calculating the exact percentage contribution of each variable\n",
    "percentage_temperature = (delta_v_temperature_squared / total_squared_changes) * 100\n",
    "percentage_pressure = (delta_v_pressure_squared / total_squared_changes) * 100\n",
    "percentage_salinity = (delta_v_salinity_squared / total_squared_changes) * 100\n",
    "                       \n",
    "# Output baseline velocities\n",
    "print(\"Baseline Velocities:\")\n",
    "print(f\"GSW with TEOS-10: {baseline_velocity_gsw_TEOS10} m/s\")\n",
    "print(f\"GSW Basic T, P, S: {baseline_velocity_gsw_basic} m/s\")\n",
    "\n",
    "# Output sensitivity results with units\n",
    "print(\"\\nGSW Method Sensitivities with TEOS-10 (m/s/PSU, m/s/°C, m/s/kPa):\")\n",
    "print(f\"Salinity: {sensitivity_S_gsw_TEOS10}, Temperature: {sensitivity_T_gsw_TEOS10}, Pressure: {sensitivity_P_gsw_TEOS10*10}\")\n",
    "print(f\"Percentage Variance explained by Salinity: {percentage_salinity} %\")\n",
    "print(f\"Percentage Variance explained by Temperature: {percentage_temperature} %\")\n",
    "print(f\"Percentage Variance explained by Pressure: {percentage_pressure} %\")\n",
    "      \n",
    "print(\"\\nChen and Millero Method Sensitivities (Basic T, P, S) (m/s/PSU, m/s/°C, m/s/kPa):\")\n",
    "print(f\"Salinity: {sensitivity_S_gsw_basic}, Temperature: {sensitivity_T_gsw_basic}, Pressure: {sensitivity_P_gsw_basic*10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Velocity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "longitude = -130.01149966\n",
    "latitude = 45.95882833\n",
    "\n",
    "# Example DataFrame columns\n",
    "T_series = np.array(combined_df4['Temperature Deg C'])\n",
    "P_series_kPa = np.array(combined_df4['Corrected Pressure (kPa)'])\n",
    "\n",
    "# If salinity is a constant\n",
    "S_constant =  np.array(combined_df4['Salinity'])  # Assuming salinity is a constant value\n",
    "\n",
    "# Call the functions with timeseries data or constants\n",
    "TEOS10_4 = velocity_timeseries_TEOS10(T_series, P_series_kPa, S_constant)  # Replace S_constant with S_series if salinity is a timeseries\n",
    "combined_df4['velocity']=TEOS10_4\n",
    "combined_df4.reset_index(inplace=True)\n",
    "\n",
    "# Example DataFrame columns\n",
    "T_series = np.array(combined_df3['Temperature Deg C'])\n",
    "P_series_kPa = np.array(combined_df3['Corrected Pressure (kPa)'])\n",
    "\n",
    "# If salinity is a constant\n",
    "S_constant =  np.array(combined_df3['Salinity'])  # Assuming salinity is a constant value\n",
    "\n",
    "# Call the functions with timeseries data or constants\n",
    "TEOS10_3 = velocity_timeseries_TEOS10(T_series, P_series_kPa, S_constant)  # Replace S_constant with S_series if salinity is a timeseries\n",
    "combined_df3['velocity']=TEOS10_3\n",
    "combined_df3.reset_index(inplace=True)\n",
    "\n",
    "# Example DataFrame columns\n",
    "T_series = np.array(combined_df2['Temperature Deg C'])\n",
    "P_series_kPa = np.array(combined_df2['Corrected Pressure (kPa)'])\n",
    "\n",
    "# If salinity is a constant\n",
    "S_constant =  np.array(combined_df2['Salinity'])  # Assuming salinity is a constant value\n",
    "\n",
    "# Call the functions with timeseries data or constants\n",
    "TEOS10_2 = velocity_timeseries_TEOS10(T_series, P_series_kPa, S_constant)  # Replace S_constant with S_series if salinity is a timeseries\n",
    "combined_df2['velocity']=TEOS10_2\n",
    "combined_df2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_series = np.array(combined_df4['Temperature Deg C'])\n",
    "T_DQZ = np.array(combined_df4['TempDQZ'])\n",
    "P_series_kPa = np.array(combined_df4['Corrected Pressure (kPa)'])\n",
    "S_constant =  np.array(combined_df4['Salinity'])  # Assuming salinity is a constant value\n",
    "TEOS1 = velocity_timeseries_TEOS10(T_series, P_series_kPa, S_constant)#T_series+0.351  # Replace S_constant with S_series if salinity is a timeseries\n",
    "combined_df4['velocity_c']=TEOS1\n",
    "TEOSDQZ= velocity_timeseries_TEOS10(T_DQZ, P_series_kPa, S_constant)\n",
    "combined_df4['velocity_dqz']=TEOSDQZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df2['Record Time'],combined_df2['Temperature Deg C'],label='2502 Temperature')\n",
    "plt.plot(combined_df3['Record Time'],combined_df3['Temperature Deg C'],label='2503 Temperature')\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['Temperature Deg C']+0.351,label='2504 Temperature (Corrected)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature Deg C')\n",
    "#plt.title('Comparison of Calculated Velocity')\n",
    "plt.legend()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['Temperature Deg C'],label='Recorded Temperature')\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['TempDQZ'],label='DQZ Temperature')\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['Temperature Deg C']+0.351,label='Corrected Temperature')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature Deg C')\n",
    "#plt.title('Comparison of Calculated Velocity')\n",
    "plt.legend()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['velocity'],label='V(T,P,S)')\n",
    "plt.plot(result_dfs['2504']['Record Time'], result_dfs['2504']['SoundSpeed'],label='Recorded Velocity')\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['velocity_dqz'],label='V(T(DQZ),P,S')\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['velocity_c'],label='V(T+0.351,P,S)',color='black')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "#plt.title('Comparison of Calculated Velocity')\n",
    "plt.legend()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df3.reset_index(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(result_dfs['2504']['Record Time'], result_dfs['2504']['SoundSpeed'],label='2504 Recorded Velocity')\n",
    "plt.plot(combined_df4['Record Time'],combined_df4['velocity_c'],label='V(T+0.351,P,S) 2504',linewidth=1)\n",
    "plt.plot(combined_df3['Record Time'],combined_df3['velocity'],label='V(T,P,S) 2504',linewidth=1)\n",
    "\n",
    "plt.plot(result_dfs['2502']['Record Time'], result_dfs['2502']['SoundSpeed'],label='2502 Recorded Velocity')\n",
    "plt.plot(combined_df2['Record Time'],combined_df2['velocity'],label='V(T,P,S) 2502',color = 'black',linewidth=1)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Velocity (m/s)')\n",
    "#plt.title('Comparison of Calculated Velocity')\n",
    "plt.legend()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_rms_error(values1, values2):\n",
    "    # Ensure the arrays are numpy arrays\n",
    "    values1 = np.array(values1)\n",
    "    values2 = np.array(values2)\n",
    "\n",
    "    # Calculate the difference\n",
    "    differences = values1 - values2\n",
    "\n",
    "    # Remove NaN values from the differences\n",
    "    differences = differences[~np.isnan(differences)]\n",
    "\n",
    "    # Calculate the mean squared error, excluding NaN values\n",
    "    mean_squared_error = np.nanmean(differences ** 2)\n",
    "\n",
    "    # Take the square root of the mean squared error\n",
    "    rms_error = np.sqrt(mean_squared_error)\n",
    "    return rms_error\n",
    "\n",
    "# Assuming combined_df4 is your DataFrame\n",
    "interp_v = combined_df4['interp_v'].values\n",
    "velocity = combined_df4['velocity_c'].values\n",
    "\n",
    "# Calculate the RMS error\n",
    "rms_error = calculate_rms_error(interp_v, velocity)\n",
    "print(f\"RMS Error: {rms_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_mean_dfs['2502_2503']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "off = [0.36,0.354,0.352,0.351, 0.35,0.348,0.345,0.34] #0\n",
    "rmserror = [0.04271085839137481,0.0212513857,0.017255038520958124,0.016665532022, 0.017196258148926, 0.0211084845, 0.030905983,0.0506384479773202]#1.5357631742372628\n",
    "plt.plot(pd.to_numeric(off),pd.to_numeric(rmserror))\n",
    "plt.xlabel('Temperature offset (C)')\n",
    "plt.ylabel('RMS (m/s)')\n",
    "#plt.title('Comparison of Calculated Velocity')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data frame creation (to be replaced with your actual data)\n",
    "# combined_df4 = pd.DataFrame({\n",
    "#     'Temperature Deg C': [/* your temperature data */],\n",
    "#     'Corrected Pressure (kPa)': [/* your pressure data */],\n",
    "#     'Salinity': [/* your salinity data */],\n",
    "#     'interp_v': [/* your interp_v data */]\n",
    "# })\n",
    "\n",
    "def velocity_timeseries_TEOS10(T_series, P_series_kPa, S_constant):\n",
    "    # Dummy function for demonstration (replace with your actual function)\n",
    "    return T_series + P_series_kPa + S_constant\n",
    "\n",
    "\n",
    "\n",
    "offsets = np.arange(0.1, 0.1, 1)  # Adjust the range and step as needed\n",
    "rms_erro = []\n",
    "\n",
    "for offset in offsets:\n",
    "    T_series = np.array(combined_df4['Temperature Deg C'])\n",
    "    P_series_kPa = np.array(combined_df4['Corrected Pressure (kPa)'])\n",
    "    S_constant = np.array(combined_df4['Salinity'])\n",
    "\n",
    "    TEOS1 = velocity_timeseries_TEOS10(T_series + offset, P_series_kPa, S_constant)\n",
    "    combined_df4['velocity_c'] = TEOS1\n",
    "\n",
    "    interp_v = combined_df4['interp_v'].values\n",
    "    velocity = combined_df4['velocity_c'].values\n",
    "\n",
    "    rms_erro = calculate_rms_error(interp_v, velocity)\n",
    "    rms_erro.append(rms_error)\n",
    "            # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    #plt.plot(offsets, rms_errors, label='RMS Error', color='blue')\n",
    "    plt.scatter([offset], [rms_erro], color='red')\n",
    "    plt.xlabel('Offset')\n",
    "    plt.ylabel('RMS Error')\n",
    "    plt.title('RMS Error for Different Offsets')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def find_best_offset(combined_df4):\n",
    "    min_rms_error = float('inf')\n",
    "    best_offset = None\n",
    "\n",
    "    # Iterate over a range of possible offsets\n",
    "    for offset in np.arange(0.1,0.01,0.5):  # Adjust the range and step as needed\n",
    "        T_series = np.array(combined_df4['Temperature Deg C'])\n",
    "        P_series_kPa = np.array(combined_df4['Corrected Pressure (kPa)'])\n",
    "        S_constant = np.array(combined_df4['Salinity'])\n",
    "\n",
    "        TEOS1 = velocity_timeseries_TEOS10(T_series + offset, P_series_kPa, S_constant)\n",
    "        combined_df4['velocity_c'] = TEOS1\n",
    "\n",
    "        interp_v = combined_df4['interp_v'].values\n",
    "        velocity = combined_df4['velocity_c'].values\n",
    "\n",
    "        rms_error = calculate_rms_error(interp_v, velocity)\n",
    "\n",
    "        if rms_error < min_rms_error:\n",
    "            min_rms_error = rms_error\n",
    "            best_offset = offset\n",
    "\n",
    "    return best_offset, min_rms_error\n",
    "\n",
    "# Find the best offset\n",
    "best_offset, min_rms_error = find_best_offset(combined_df4)\n",
    "print(f\"Best Offset: {best_offset}, Min RMS Error: {min_rms_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Harmonic Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming combined_df, combined_df1, and combined_df2 are already defined\n",
    "\n",
    "# Function to set 'Record Time' as datetime index if it's not already\n",
    "def set_datetime_index(df):\n",
    "    if 'Record Time' in df.columns:\n",
    "        df['Record Time'] = pd.to_datetime(df['Record Time'])\n",
    "        df.set_index('Record Time', inplace=True)\n",
    "    elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Applying the function to each DataFrame\n",
    "set_datetime_index(combined_df4)\n",
    "set_datetime_index(combined_df3)\n",
    "set_datetime_index(combined_df2)\n",
    "\n",
    "# Define a function to compute the harmonic mean for a pair of DataFrames\n",
    "def compute_harmonic_mean(df1, df2, column='velocity'):\n",
    "    # Intersect the indices (Record Time) of both dataframes\n",
    "    common_index = df1.index.intersection(df2.index)\n",
    "    # Select only the common timestamps\n",
    "    aligned_df1 = df1.loc[common_index]\n",
    "    aligned_df2 = df2.loc[common_index]\n",
    "    # Ensure no zero or negative values as they are invalid for harmonic mean\n",
    "    aligned_df1[column] = aligned_df1[column].clip(lower=0.0001)\n",
    "    aligned_df2[column] = aligned_df2[column].clip(lower=0.0001)\n",
    "    # Compute the harmonic mean\n",
    "    return common_index, hmean([aligned_df1[column], aligned_df2[column]], axis=0)\n",
    "\n",
    "# Define a function to compute the harmonic mean for a pair of DataFrames\n",
    "def compute_harmonic_mean(df1, df2, column='velocity'):\n",
    "    # Intersect the indices (Record Time) of both DataFrames\n",
    "    common_index = df1.index.intersection(df2.index)\n",
    "\n",
    "    # Select only the common timestamps\n",
    "    aligned_df1 = df1.loc[common_index, [column]].copy()\n",
    "    aligned_df2 = df2.loc[common_index, [column]].copy()\n",
    "\n",
    "    # Drop any remaining NaNs after alignment\n",
    "    aligned_df1.dropna(inplace=True)\n",
    "    aligned_df2.dropna(inplace=True)\n",
    "\n",
    "    # Ensure both have the same length after dropping NaNs\n",
    "    min_length = min(len(aligned_df1), len(aligned_df2))\n",
    "    aligned_df1 = aligned_df1.iloc[:min_length]\n",
    "    aligned_df2 = aligned_df2.iloc[:min_length]\n",
    "\n",
    "    # Ensure no zero or negative values as they are invalid for harmonic mean\n",
    "    aligned_df1[column] = aligned_df1[column].clip(lower=0.0001)\n",
    "    aligned_df2[column] = aligned_df2[column].clip(lower=0.0001)\n",
    "\n",
    "    # Convert to NumPy arrays for compatibility with `hmean()`\n",
    "    values1 = aligned_df1[column].to_numpy()\n",
    "    values2 = aligned_df2[column].to_numpy()\n",
    "\n",
    "    # Compute the harmonic mean\n",
    "    harmonic_mean_values = hmean(np.vstack([values1, values2]), axis=0)\n",
    "\n",
    "    return common_index[:min_length], harmonic_mean_values\n",
    "\n",
    "# Compute the harmonic mean for each pair\n",
    "index_2504_2503, harmonic_mean_2504_2503 = compute_harmonic_mean(combined_df4, combined_df3)\n",
    "index_2504_2502, harmonic_mean_2504_2502 = compute_harmonic_mean(combined_df4, combined_df2)\n",
    "index_2503_2502, harmonic_mean_2503_2502 = compute_harmonic_mean(combined_df3, combined_df2)\n",
    "\n",
    "\n",
    "#Assuming the harmonic means and their corresponding indices have been calculated\n",
    "harmonic_means = {\n",
    "    '2502_2503': (index_2503_2502, harmonic_mean_2503_2502),\n",
    "    '2502_2504': (index_2504_2502, harmonic_mean_2504_2502),\n",
    "    '2503_2504': (index_2504_2503, harmonic_mean_2504_2503)\n",
    "}\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "harmonic_df_dict = {}\n",
    "\n",
    "for key, (index, values) in harmonic_means.items():\n",
    "    # Create a DataFrame without setting 'Record Time' as the index\n",
    "    df = pd.DataFrame({'Record Time': index, 'HMean': values})\n",
    "    harmonic_df_dict[key] = df\n",
    "\n",
    "# Now, harmonic_df_dict contains DataFrames for each harmonic mean series under the specified keys\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "#plt.plot(index_2504_2503, harmonic_mean_2504_2503, label='Harmonic Mean 2504-2503')\n",
    "#plt.plot(index_2504_2502, harmonic_mean_2504_2502, label='Harmonic Mean 2504-2502')\n",
    "plt.plot(index_2503_2502, harmonic_mean_2503_2502, label='TEOS 2503-2502')\n",
    "plt.plot(harmonic_mean_dfs['2502_2503']['Record Time'],harmonic_mean_dfs['2502_2503']['SoundSpeed'],label='Velo 2503-2502')\n",
    "plt.plot(harmonic_mean_dfs['2503_2504']['Record Time'],harmonic_mean_dfs['2503_2504']['SoundSpeed'],label='Velo 2503-2504')\n",
    "\n",
    "#plt.title('Harmonic Mean Velocity between 2504-2502')\n",
    "plt.xlabel('Record Time')\n",
    "plt.ylabel('Harmonic Mean Velocity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_mean_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_df = df_dict['2504']['BSL']\n",
    "bsl_df['RangeAddress'] = bsl_df['RangeAddress'].astype(int).astype(str)\n",
    "bsl_df['Range(ms)'] = pd.to_numeric(bsl_df['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2503_df = bsl_df[bsl_df['RangeAddress'] == '2503'].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(bsl_2503_df['Record Time'],bsl_2503_df['Range(ms)'],s=1)\n",
    "plt.title('Two way travel time from 2504-2503 ')\n",
    "plt.xlabel('Record Time')\n",
    "plt.ylabel('Range (ms)')\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(filtered_bsl_2502_copy['Record Time'],filtered_bsl_2502_copy['Range(ms)'],s=1)\n",
    "plt.title('Two way travel time from 2504-2502 ')\n",
    "plt.xlabel('Record Time')\n",
    "plt.ylabel('Range (ms)')\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2504 to east and west"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DataFrames based on the 'RangeAddress' column values\n",
    "bsl_df = df_dict['2504']['BSL']\n",
    "bsl_df['RangeAddress'] = bsl_df['RangeAddress'].astype(int).astype(str)\n",
    "bsl_df['Range(ms)'] = pd.to_numeric(bsl_df['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2501_df = bsl_df[bsl_df['RangeAddress'] == '2501'].copy()\n",
    "bsl_2502_df = bsl_df[bsl_df['RangeAddress'] == '2502'].copy()\n",
    "bsl_2503_df = bsl_df[bsl_df['RangeAddress'] == '2503'].copy()\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2502_df['Range(ms)'].quantile(0.15)\n",
    "Q3 = bsl_2502_df['Range(ms)'].quantile(0.85)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "total_rows1 = len(bsl_2502_df)\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_bsl_2502_df = bsl_2502_df[(bsl_2502_df['Range(ms)'] >= lower_bound) & (bsl_2502_df['Range(ms)'] <= upper_bound)]\n",
    "outside_values_df1 = bsl_2502_df[(bsl_2502_df['Range(ms)'] < lower_bound) | (bsl_2502_df['Range(ms)'] > upper_bound)]\n",
    "\n",
    "# Number of rows after removing outliers\n",
    "rows_after_filtering1 = len(filtered_bsl_2502_df)\n",
    "# Calculate the number of outlier rows\n",
    "num_outliers1 = total_rows1 - rows_after_filtering1\n",
    "# Calculate the percentage of outliers\n",
    "percent_outliers_2504_2502 = (num_outliers1 / total_rows1) * 100\n",
    "\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2503_df['Range(ms)'] = pd.to_numeric(bsl_2503_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "total_rows = len(bsl_2503_df)\n",
    "\n",
    "# Filter out data points with 'Range(ms)' values less than 2618 from the original bsl_2503_df\n",
    "filtered_bsl_2503_df = bsl_2503_df[bsl_2503_df['Range(ms)'] >= 2618]\n",
    "outside_values_df2 = bsl_2503_df[bsl_2503_df['Range(ms)'] < 2618]\n",
    "\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2502_df['Range(ms)'] = pd.to_numeric(bsl_2502_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "# Creating explicit copies of the DataFrames to avoid the warning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_df.copy()\n",
    "\n",
    "# Create explicit copies to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2502_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2504'], 'Record Time')\n",
    "\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2502_copy timestamps\n",
    "filtered_bsl_2502_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Calculated Distance (m)'] = filtered_bsl_2502_copy['Interpolated Sound Speed'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "\n",
    "filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Harmonic Mean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Harmonic Distance (m)'] = filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "\n",
    "\n",
    "R2504_2502 = filtered_bsl_2502_copy\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create an explicit copy of filtered_bsl_2503_df to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2503_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_mean_dfs['2503_2504'], 'Record Time')\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2503_copy timestamps\n",
    "filtered_bsl_2503_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2503_copy['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2503_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2503_2504']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2503_copy['Calculated Distance (m)'] = filtered_bsl_2503_copy['Interpolated Sound Speed'] * ((filtered_bsl_2503_copy['Range(ms)']-filtered_bsl_2503_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "\n",
    "\n",
    "# Interpolate Harmonic Mean Velocity onto the filtered_bsl_2503_copy timestamps\n",
    "filtered_bsl_2503_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2503_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2503_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2503_2504']['SoundSpeed']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated Harmonic Mean Velocity\n",
    "# Remember to divide the time (Range - TAT) by 2000 to convert ms to seconds and for two-way travel\n",
    "filtered_bsl_2503_copy['Harmonic Distance (m)'] = filtered_bsl_2503_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2503_copy['Range(ms)'] - filtered_bsl_2503_copy['TAT(ms)']) / 2000)\n",
    "\n",
    "R2504_2503 = filtered_bsl_2503_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2504_2503['Record Time'] = pd.to_datetime(R2504_2503['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "R2504_2503 = R2504_2503.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "R2504_2503 = R2504_2503.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "R2504_2503['Moving Average'] = (\n",
    "    R2504_2503['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100\n",
    "\n",
    "# Reset index for plotting\n",
    "R2504_2503 = R2504_2503.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(R2504_2503['Record Time'], R2504_2503['Harmonic Distance (m)']*100, '.', label=\"Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(R2504_2503['Record Time'], \n",
    "         R2504_2503['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.ylim(176650,176690)\n",
    "plt.title(\"Central -> West Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2504_2503['Record Time'] = pd.to_datetime(R2504_2503['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2504_2503.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = R2504_2503.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"Central-West Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2504_2502['Record Time'] = pd.to_datetime(R2504_2502['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "R2504_2502 = R2504_2502.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "R2504_2502 = R2504_2502.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "R2504_2502['Moving Average'] = (\n",
    "    R2504_2502['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100\n",
    "\n",
    "# Reset index for plotting\n",
    "R2504_2502 = R2504_2502.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(R2504_2502['Record Time'], R2504_2502['Harmonic Distance (m)']*100, '.', label=\"Velocimeter Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(R2504_2502['Record Time'], \n",
    "         R2504_2502['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Harmonic Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.ylim(164200,164225)\n",
    "plt.title(\"Central -> East Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2504_2502['Record Time'] = pd.to_datetime(R2504_2502['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2504_2502.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = R2504_2502.resample('30D', on='Record Time').agg({\n",
    "    'Calculated Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"Central-East Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2503 to central and east"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DataFrames based on the 'RangeAddress' column values\n",
    "bsl_df = df_dict['2503']['BSL']\n",
    "bsl_df['RangeAddress'] = bsl_df['RangeAddress'].astype(int).astype(str)\n",
    "bsl_df['Range(ms)'] = pd.to_numeric(bsl_df['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2502_df = bsl_df[bsl_df['RangeAddress'] == '2502'].copy()\n",
    "bsl_2504_df = bsl_df[bsl_df['RangeAddress'] == '2504'].copy()\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2502_df['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2502_df['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_bsl_2502_df = bsl_2502_df[(bsl_2502_df['Range(ms)'] >= lower_bound) & (bsl_2502_df['Range(ms)'] <= upper_bound)]\n",
    "outside_values_df1 = bsl_2502_df[(bsl_2502_df['Range(ms)'] < lower_bound) | (bsl_2502_df['Range(ms)'] > upper_bound)]\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2502_df['Range(ms)'] = pd.to_numeric(bsl_2502_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2504_df['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2504_df['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "# Filter out data points with 'Range(ms)' values using IQR\n",
    "filtered_bsl_2504_df = bsl_2504_df[(bsl_2504_df['Range(ms)'] >= lower_bound) & (bsl_2504_df['Range(ms)'] <= upper_bound)]\n",
    "outside_values_df2 = bsl_2504_df[(bsl_2504_df['Range(ms)'] < lower_bound) | (bsl_2504_df['Range(ms)'] > upper_bound)]\n",
    "\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2504_df['Range(ms)'] = pd.to_numeric(bsl_2504_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "# Creating explicit copies of the DataFrames to avoid the warning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "filtered_bsl_2504_copy = filtered_bsl_2504_df.copy()\n",
    "\n",
    "# Create explicit copies to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2502_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2503'], 'Record Time')\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2502_copy timestamps\n",
    "filtered_bsl_2502_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Calculated Distance (m)'] = filtered_bsl_2502_copy['Interpolated Sound Speed'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "# Display the top rows of the updated dataframe\n",
    "filtered_bsl_2502_copy[['Record Time', 'Range(ms)', 'Interpolated Sound Speed', 'Calculated Distance (m)']].head()\n",
    "\n",
    "##############\n",
    "filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Harmonic Mean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Harmonic Distance (m)'] = filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "##############\n",
    "R2503_2502 = filtered_bsl_2502_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "ensure_datetime(filtered_bsl_2504_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2504'], 'Record Time')\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2502_copy timestamps\n",
    "filtered_bsl_2504_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2504_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2504_copy['Calculated Distance (m)'] = filtered_bsl_2504_copy['Interpolated Sound Speed'] * ((filtered_bsl_2504_copy['Range(ms)']-filtered_bsl_2504_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "# Display the top rows of the updated dataframe\n",
    "filtered_bsl_2504_copy[['Record Time', 'Range(ms)', 'Interpolated Sound Speed', 'Calculated Distance (m)']].head()\n",
    "\n",
    "##############\n",
    "filtered_bsl_2504_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2504_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Harmonic Mean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2504_copy['Harmonic Distance (m)'] = filtered_bsl_2504_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2504_copy['Range(ms)']-filtered_bsl_2504_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "##############\n",
    "R2503_2504 = filtered_bsl_2504_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Interpolation for 2504\n",
    "#ensure_datetime(filtered_bsl_2504_copy, 'Record Time')\n",
    "#ensure_datetime(harmonic_df_dict['2503_2504'], 'Record Time')\n",
    "\n",
    "#filtered_bsl_2504_copy['Interpolated Sound Speed'] = np.interp(\n",
    "#    filtered_bsl_2504_copy['Record Time'].astype(np.int64),\n",
    "#    harmonic_df_dict['2503_2504']['Record Time'].view(np.int64),\n",
    "#    harmonic_df_dict['2503_2504']['HMean']\n",
    "#)\n",
    "#\n",
    "#filtered_bsl_2504_copy['Harmonic Distance (m)'] = filtered_bsl_2504_copy['Interpolated Sound Speed'] * ((filtered_bsl_2504_copy['Range(ms)']-pd.to_numeric(filtered_bsl_2504_copy['TAT(ms)'])) / 2000)\n",
    "\n",
    "##    Now we do this for central\n",
    "\n",
    "#R2503_2504 = filtered_bsl_2504_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2502['Record Time'] = pd.to_datetime(R2503_2502['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "R2503_2502 = R2503_2502.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "R2503_2502 = R2503_2502.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "R2503_2502['Moving Average'] = (\n",
    "    R2503_2502['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100\n",
    "\n",
    "# Reset index for plotting\n",
    "R2503_2502 = R2503_2502.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(R2503_2502['Record Time'], R2503_2502['Harmonic Distance (m)']*100, '.', label=\"Velocimeter Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(R2503_2502['Record Time'], \n",
    "         R2503_2502['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Harmonic Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.ylim(326200,326245)\n",
    "plt.title(\"West -> East Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2502['Record Time'] = pd.to_datetime(R2503_2502['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2503_2502.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = R2503_2502.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"West-East Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2504['Record Time'] = pd.to_datetime(R2503_2504['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "R2503_2504 = R2503_2504.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "R2503_2504 = R2503_2504.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "R2503_2504['Moving Average'] = (\n",
    "    R2503_2504['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100\n",
    "\n",
    "# Reset index for plotting\n",
    "R2503_2504 = R2503_2504.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(R2503_2504['Record Time'], R2503_2504['Harmonic Distance (m)']*100, '.', label=\"Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(R2503_2504['Record Time'], \n",
    "         R2503_2504['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.ylim(176600,176630)\n",
    "plt.title(\"West -> Central Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2504['Record Time'] = pd.to_datetime(R2503_2504['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2503_2504.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = R2503_2504.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"West-Central Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2502 to west and central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract DataFrames based on the 'RangeAddress' column values\n",
    "bsl_dfs = df_dict['2502']['BSL']\n",
    "bsl_dfs['RangeAddress'] = bsl_dfs['RangeAddress'].astype(int).astype(str)\n",
    "bsl_dfs['Range(ms)'] = pd.to_numeric(bsl_dfs['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2503_dfs = bsl_dfs[bsl_dfs['RangeAddress'] == '2503'].copy()\n",
    "bsl_2504_dfs = bsl_dfs[bsl_dfs['RangeAddress'] == '2504'].copy()\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2503_dfs['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2503_dfs['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_bsl_2503_dfs = bsl_2503_dfs[(bsl_2503_dfs['Range(ms)'] >= 4636.2) & (bsl_2503_dfs['Range(ms)'] <= 4638)]\n",
    "outside_values_df2 = bsl_2503_dfs[(bsl_2503_dfs['Range(ms)'] < 4636.2) & (bsl_2503_dfs['Range(ms)'] > 4638)]\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2503_dfs['Range(ms)'] = pd.to_numeric(bsl_2503_dfs['Range(ms)'], errors='coerce')\n",
    "bsl_2503_dfs['TAT(ms)'] = pd.to_numeric(bsl_2503_dfs['TAT(ms)'], errors='coerce')\n",
    "Q1 = bsl_2504_dfs['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2504_dfs['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "filtered_bsl_2504_dfs = bsl_2504_dfs[(bsl_2504_dfs['Range(ms)'] >= 2400)]\n",
    "outside_values_df2 = bsl_2504_dfs[(bsl_2504_dfs['Range(ms)'] < 2400)]\n",
    "\n",
    "bsl_2504_dfs['Range(ms)'] = pd.to_numeric(bsl_2504_dfs['Range(ms)'], errors='coerce')\n",
    "bsl_2504_dfs['TAT(ms)'] = pd.to_numeric(bsl_2504_dfs['TAT(ms)'], errors='coerce')\n",
    "\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_dfs.copy()\n",
    "filtered_bsl_2504_copy = filtered_bsl_2504_dfs.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2503_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_mean_dfs['2502_2503'], 'Record Time')\n",
    "\n",
    "# Interpolation for 2503\n",
    "filtered_bsl_2503_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2503_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['HMean']\n",
    ")\n",
    "\n",
    "filtered_bsl_2503_copy['Harmonic Distance (m)'] = filtered_bsl_2503_copy['Interpolated Sound Speed'] * ((filtered_bsl_2503_copy['Range(ms)']-pd.to_numeric(filtered_bsl_2503_copy['TAT(ms)'])) / 2000)\n",
    "\n",
    "R2502_2503 = filtered_bsl_2503_copy\n",
    "\n",
    "# Interpolation for 2504\n",
    "ensure_datetime(filtered_bsl_2504_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2504'], 'Record Time')\n",
    "\n",
    "filtered_bsl_2504_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2504_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['HMean']\n",
    ")\n",
    "\n",
    "filtered_bsl_2504_copy['Harmonic Distance (m)'] = filtered_bsl_2504_copy['Interpolated Sound Speed'] * ((filtered_bsl_2504_copy['Range(ms)']-pd.to_numeric(filtered_bsl_2504_copy['TAT(ms)'])) / 2000)\n",
    "\n",
    "##    Now we do this for central\n",
    "\n",
    "R2502_2504 = filtered_bsl_2504_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2503['Record Time'] = pd.to_datetime(R2502_2503['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "R2502_2503 = R2502_2503.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "R2502_2503 = R2502_2503.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "R2502_2503['Moving Average'] = (\n",
    "    R2502_2503['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100\n",
    "\n",
    "# Reset index for plotting\n",
    "R2502_2503 = R2502_2503.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(R2502_2503['Record Time'], R2502_2503['Harmonic Distance (m)']*100, '.', label=\"Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(R2502_2503['Record Time'], \n",
    "         R2502_2503['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.ylim(326000,326040)\n",
    "plt.title(\"East -> west Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2503['Record Time'] = pd.to_datetime(R2502_2503['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2502_2503.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = R2502_2503.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"East-West Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2502_2504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2504['Record Time'] = pd.to_datetime(R2502_2504['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "R2502_2504 = R2502_2504.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "R2502_2504 = R2502_2504.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "R2502_2504['Moving Average'] = (\n",
    "    R2502_2504['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100\n",
    "\n",
    "# Reset index for plotting\n",
    "R2502_2504 = R2502_2504.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(R2502_2504['Record Time'], R2502_2504['Harmonic Distance (m)']*100, '.', label=\"Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(R2502_2504['Record Time'], \n",
    "         R2502_2504['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.ylim(164200,164222)\n",
    "plt.title(\"East -> Central Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2504['Record Time'] = pd.to_datetime(R2502_2504['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2502_2504.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = R2502_2504.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"East-Central Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DataFrames based on the 'RangeAddress' column values\n",
    "bsl_df = df_dict['2504']['BSL']\n",
    "bsl_df['RangeAddress'] = bsl_df['RangeAddress'].astype(int).astype(str)\n",
    "bsl_df['Range(ms)'] = pd.to_numeric(bsl_df['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2501_df = bsl_df[bsl_df['RangeAddress'] == '2501'].copy()\n",
    "bsl_2502_df = bsl_df[bsl_df['RangeAddress'] == '2502'].copy()\n",
    "bsl_2503_df = bsl_df[bsl_df['RangeAddress'] == '2503'].copy()\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2502_df['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2502_df['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "total_rows1 = len(bsl_2502_df)\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_bsl_2502_df = bsl_2502_df[(bsl_2502_df['Range(ms)'] >= lower_bound) & (bsl_2502_df['Range(ms)'] <= upper_bound)]\n",
    "outside_values_df1 = bsl_2502_df[(bsl_2502_df['Range(ms)'] < lower_bound) | (bsl_2502_df['Range(ms)'] > upper_bound)]\n",
    "\n",
    "# Number of rows after removing outliers\n",
    "rows_after_filtering1 = len(filtered_bsl_2502_df)\n",
    "# Calculate the number of outlier rows\n",
    "num_outliers1 = total_rows1 - rows_after_filtering1\n",
    "# Calculate the percentage of outliers\n",
    "percent_outliers_2504_2502 = (num_outliers1 / total_rows1) * 100\n",
    "\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2503_df['Range(ms)'] = pd.to_numeric(bsl_2503_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "total_rows = len(bsl_2503_df)\n",
    "\n",
    "# Filter out data points with 'Range(ms)' values less than 2618 from the original bsl_2503_df\n",
    "filtered_bsl_2503_df = bsl_2503_df[bsl_2503_df['Range(ms)'] >= 2618]\n",
    "outside_values_df2 = bsl_2503_df[bsl_2503_df['Range(ms)'] < 2618]\n",
    "\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2502_df['Range(ms)'] = pd.to_numeric(bsl_2502_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "# Creating explicit copies of the DataFrames to avoid the warning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_df.copy()\n",
    "\n",
    "# Create explicit copies to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2502_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2504'], 'Record Time')\n",
    "\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2502_copy timestamps\n",
    "filtered_bsl_2502_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Calculated Distance (m)'] = filtered_bsl_2502_copy['Interpolated Sound Speed'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "\n",
    "##############\n",
    "filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Harmonic Mean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Harmonic Distance (m)'] = filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "##############\n",
    "\n",
    "\n",
    "# Create an explicit copy of filtered_bsl_2503_df to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2503_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_mean_dfs['2503_2504'], 'Record Time')\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2503_copy timestamps\n",
    "filtered_bsl_2503_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2503_copy['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2503_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2503_2504']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2503_copy['Calculated Distance (m)'] = filtered_bsl_2503_copy['Interpolated Sound Speed'] * ((filtered_bsl_2503_copy['Range(ms)']-filtered_bsl_2503_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "\n",
    "\n",
    "# Interpolate Harmonic Mean Velocity onto the filtered_bsl_2503_copy timestamps\n",
    "filtered_bsl_2503_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2503_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2503_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2503_2504']['SoundSpeed']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated Harmonic Mean Velocity\n",
    "# Remember to divide the time (Range - TAT) by 2000 to convert ms to seconds and for two-way travel\n",
    "filtered_bsl_2503_copy['Harmonic Distance (m)'] = filtered_bsl_2503_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2503_copy['Range(ms)'] - filtered_bsl_2503_copy['TAT(ms)']) / 2000)\n",
    "\n",
    "# Calculate centered distances by subtracting the mean\n",
    "#baseline_2504_2502 = filtered_bsl_2502_copy['Calculated Distance (m)'] - filtered_bsl_2502_copy['Calculated Distance (m)'].mean()\n",
    "#baseline_2504_2503 = filtered_bsl_2503_copy['Calculated Distance (m)'] - filtered_bsl_2503_copy['Calculated Distance (m)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy is defined and has necessary columns\n",
    "\n",
    "# Prepare data for plotting (convert to Unix timestamp and drop NaNs)\n",
    "filtered_bsl_2502_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2502_copy['Record Time']).astype(np.int64) // 10**9\n",
    "filtered_bsl_2502_copy.dropna(subset=['Calculated Distance (m)', 'Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit quadratic and linear polynomials for 'Calculated Distance (m)'\n",
    "coefficients_poly_calc = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 2)\n",
    "coefficients_linear_calc = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 1)\n",
    "polynomial_calc = np.poly1d(coefficients_poly_calc)\n",
    "linear_calc = np.poly1d(coefficients_linear_calc)\n",
    "\n",
    "# Fit quadratic and linear polynomials for 'Harmonic Distance (m)'\n",
    "coefficients_poly_harmonic = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Harmonic Distance (m)'], 2)\n",
    "coefficients_linear_harmonic = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Harmonic Distance (m)'], 1)\n",
    "polynomial_harmonic = np.poly1d(coefficients_poly_harmonic)\n",
    "linear_harmonic = np.poly1d(coefficients_linear_harmonic)\n",
    "\n",
    "# Generate data points for the polynomial and linear lines\n",
    "x_values = np.linspace(filtered_bsl_2502_copy['Record Time Numeric'].min(), filtered_bsl_2502_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly_calc = polynomial_calc(x_values)\n",
    "y_linear_calc = linear_calc(x_values)\n",
    "y_poly_harmonic = polynomial_harmonic(x_values)\n",
    "y_linear_harmonic = linear_harmonic(x_values)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_dates = pd.to_datetime(x_values, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data for Calculated Distance\n",
    "plt.plot(filtered_bsl_2502_copy['Record Time'], filtered_bsl_2502_copy['Calculated Distance (m)'], '.', label=\"TEOS-10 Distance\", color='blue')\n",
    "# Original data for Harmonic Distance\n",
    "plt.plot(filtered_bsl_2502_copy['Record Time'], filtered_bsl_2502_copy['Harmonic Distance (m)'], '.', label=\"Velocimeter Distance\", color='green')\n",
    "\n",
    "\n",
    "# Polynomial fit for Calculated Distance\n",
    "plt.plot(x_dates, y_poly_calc, label=\"Quadratic Fit - TEOS10\", color='Red')\n",
    "\n",
    "#Polynomial fit for Harmonic Distance\n",
    "plt.plot(x_dates, y_poly_harmonic, label=\"Quadratic Fit - Velocimeter\", color='orange')\n",
    "\n",
    "# Linear fit for Calculated Distance\n",
    "#plt.plot(x_dates, y_linear_calc, label=\"Linear Fit - TEOS10\", color='black')\n",
    "# Linear fit for Harmonic Distance\n",
    "#plt.plot(x_dates, y_linear_harmonic, label=\"Linear Fit - Velocimeter\", color='purple')\n",
    "\n",
    "plt.title(\"Distance 2504-2502 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_copy['Harmonic Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# Calculate residuals and standard deviation for Calculated Distance\n",
    "filtered_bsl_2502_copy['Residuals Calc'] = filtered_bsl_2502_copy['Calculated Distance (m)'] - polynomial_calc(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "std_dev_calc = filtered_bsl_2502_copy['Residuals Calc'].std()\n",
    "\n",
    "# Filter out outliers for Calculated Distance\n",
    "filtered_bsl_2502_no_outliers_calc = filtered_bsl_2502_copy[abs(filtered_bsl_2502_copy['Residuals Calc']) <= 3 * std_dev_calc]\n",
    "\n",
    "# Calculate residuals and standard deviation for Harmonic Distance\n",
    "filtered_bsl_2502_copy['Residuals Harmonic'] = filtered_bsl_2502_copy['Harmonic Distance (m)'] - polynomial_harmonic(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "std_dev_harmonic = filtered_bsl_2502_copy['Residuals Harmonic'].std()\n",
    "\n",
    "# Filter out outliers for Harmonic Distance\n",
    "filtered_bsl_2502_no_outliers_harmonic = filtered_bsl_2502_copy[abs(filtered_bsl_2502_copy['Residuals Harmonic']) <= 3 * std_dev_harmonic]\n",
    "\n",
    "# Plotting with outliers removed\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Data for Calculated Distance without outliers\n",
    "plt.plot(filtered_bsl_2502_no_outliers_calc['Record Time'], filtered_bsl_2502_no_outliers_calc['Calculated Distance (m)']*100-164200, '.', label=\"TEOS-10 Distance\", color='grey')\n",
    "\n",
    "# Data for Harmonic Distance without outliers\n",
    "plt.plot(filtered_bsl_2502_no_outliers_harmonic['Record Time'], filtered_bsl_2502_no_outliers_harmonic['Harmonic Distance (m)']*100-164200, '.', label=\"Velocimeter Distance\", color='salmon')\n",
    "\n",
    "# Polynomial fit for Calculated Distance\n",
    "plt.plot(x_dates, y_poly_calc*100-164200, label=\"Quadratic Fit - TEOS10\", color='black',linewidth=3)\n",
    "\n",
    "# Polynomial fit for Harmonic Distance\n",
    "plt.plot(x_dates, y_poly_harmonic*100-164200, label=\"Quadratic Fit - Velocimeter\", color='red',linewidth=3)\n",
    "\n",
    "# Linear fit for Calculated Distance\n",
    "#plt.plot(x_dates, y_linear_calc, label=\"Linear Fit - TEOS10\", color='black')\n",
    "\n",
    "# Linear fit for Harmonic Distance\n",
    "#plt.plot(x_dates, y_linear_harmonic, label=\"Linear Fit - Velocimeter\", color='purple')\n",
    "\n",
    "plt.title(\"Distance 2504-2502 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (cm)\")\n",
    "plt.legend(loc='lower right')#, bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "# Set y-axis to use scientific notation\n",
    "# Force scientific notation on the y-axis\n",
    "#ax.yaxis.set_major_formatter(mticker.ScalarFormatter(useOffset=True))\n",
    "#ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "#ax.text(0.0, 1.02, '× 164200', transform=ax.transAxes, fontsize=10, verticalalignment='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "filtered_bsl_2502_no_outliers_harmonic['Record Time'] = pd.to_datetime(filtered_bsl_2502_no_outliers_harmonic['Record Time'])\n",
    "\n",
    "# Sort by time to ensure proper rolling calculations\n",
    "filtered_bsl_2502_no_outliers_harmonic = filtered_bsl_2502_no_outliers_harmonic.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index\n",
    "filtered_bsl_2502_no_outliers_harmonic = filtered_bsl_2502_no_outliers_harmonic.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day rolling mean\n",
    "filtered_bsl_2502_no_outliers_harmonic['Moving Average'] = (\n",
    "    filtered_bsl_2502_no_outliers_harmonic['Harmonic Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100 - 164200\n",
    "\n",
    "# Reset index for plotting\n",
    "filtered_bsl_2502_no_outliers_harmonic = filtered_bsl_2502_no_outliers_harmonic.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(filtered_bsl_2502_no_outliers_harmonic['Record Time'], filtered_bsl_2502_no_outliers_harmonic['Harmonic Distance (m)']*100-164200, '.', label=\"Velocimeter Distance\", color='salmon')\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(filtered_bsl_2502_no_outliers_harmonic['Record Time'], \n",
    "         filtered_bsl_2502_no_outliers_harmonic['Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Harmonic Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.title(\"East-Central Distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EastCentral=filtered_bsl_2502_no_outliers_harmonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_no_outliers_calc['Harmonic Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2502_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2502_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2502_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients_poly = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients_poly)\n",
    "\n",
    "# Fit a linear polynomial (1st degree)\n",
    "coefficients_linear = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 1)\n",
    "linear = np.poly1d(coefficients_linear)\n",
    "\n",
    "# Generate data points for the polynomial and linear lines\n",
    "x_values = np.linspace(filtered_bsl_2502_copy['Record Time Numeric'].min(), filtered_bsl_2502_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_values)\n",
    "y_linear = linear(x_values)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_dates = pd.to_datetime(x_values, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2502_copy['Record Time'], filtered_bsl_2502_copy['Calculated Distance (m)'], '.', label=\"Distance 2504-2502\", color='blue')\n",
    "# Polynomial fit\n",
    "plt.plot(x_dates, y_poly, label=\"Quadratic Fit\", color='red')\n",
    "# Linear fit\n",
    "plt.plot(x_dates, y_linear, label=\"Linear Fit\", color='black')\n",
    "\n",
    "plt.title(\"Distance 2504-2502 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2502_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2502_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2502_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients_poly = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients_poly)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = filtered_bsl_2502_copy['Calculated Distance (m)'] - polynomial(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "\n",
    "# Standard deviation of residuals\n",
    "std_dev = np.std(residuals)\n",
    "\n",
    "# Filter out data points that are more than 3 standard deviations from the polynomial fit\n",
    "filtered_data = filtered_bsl_2502_copy[np.abs(residuals) <= 3 * std_dev]\n",
    "\n",
    "# Fit a linear polynomial (1st degree) to the filtered data\n",
    "coefficients_linear = np.polyfit(filtered_data['Record Time Numeric'], filtered_data['Calculated Distance (m)'], 1)\n",
    "linear = np.poly1d(coefficients_linear)\n",
    "\n",
    "# Generate data points for the polynomial and linear lines\n",
    "x_values = np.linspace(filtered_data['Record Time Numeric'].min(), filtered_data['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_values)\n",
    "y_linear = linear(x_values)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_dates = pd.to_datetime(x_values, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data without outliers\n",
    "plt.plot(filtered_data['Record Time'], filtered_data['Calculated Distance (m)'], '.', label=\"Filtered Distance 2504-2502\", color='blue')\n",
    "# Polynomial fit\n",
    "plt.plot(x_dates, y_poly, label=\"Quadratic Fit\", color='red')\n",
    "# Linear fit\n",
    "plt.plot(x_dates, y_linear, label=\"Linear Fit\", color='green')\n",
    "\n",
    "plt.title(\"Distance 2504-2502 Timeseries (Outliers Removed)\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation and mean of the distances\n",
    "std_dev_distance = np.std(filtered_bsl_2502_copy['Calculated Distance (m)'])\n",
    "mean_distance = np.mean(filtered_bsl_2502_copy['Calculated Distance (m)'])\n",
    "\n",
    "# Convert standard deviation to centimeters and mean distance to kilometers\n",
    "std_dev_cm = std_dev_distance * 100  # Convert meters to centimeters\n",
    "mean_distance_km = mean_distance / 1000  # Convert meters to kilometers\n",
    "\n",
    "# Calculate scatter in cm/km\n",
    "scatter_cm_per_km = std_dev_cm / mean_distance_km\n",
    "\n",
    "print(f\"Scatter: {scatter_cm_per_km} cm/km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy and filtered_data are defined and have 'Calculated Distance (m)'\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree) on filtered_bsl_2502_copy\n",
    "coefficients = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 2)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate the polynomial values\n",
    "y_poly = polynomial(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "\n",
    "# Calculate residuals (misfits) for filtered_bsl_2502_copy\n",
    "residuals_bsl = filtered_bsl_2502_copy['Calculated Distance (m)'] - y_poly\n",
    "\n",
    "# Assuming residuals calculation for filtered_data is similar to above\n",
    "# If not, replace with appropriate calculation\n",
    "residuals_filtered = filtered_data['Calculated Distance (m)'] - polynomial(filtered_data['Record Time Numeric'])\n",
    "\n",
    "# Plotting the histogram of distances for both datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for filtered_bsl_2502_copy distances\n",
    "plt.hist(filtered_bsl_2502_copy['Calculated Distance (m)'], bins=30, color='blue', alpha=0.5, label='Original Data')\n",
    "\n",
    "# Histogram for filtered_data distances\n",
    "plt.hist(filtered_data['Calculated Distance (m)'], bins=30, color='green', alpha=0.5, label='Filtered Data')\n",
    "\n",
    "plt.title(\"Histogram of Distances for 2504-2502 (Original and Filtered)\")\n",
    "plt.xlabel(\"Distance (m)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2503_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2503_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2503_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2503_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2503_copy['Record Time Numeric'], filtered_bsl_2503_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Generate data points for the polynomial line\n",
    "x_poly = np.linspace(filtered_bsl_2503_copy['Record Time Numeric'].min(), filtered_bsl_2503_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_poly)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_poly_dates = pd.to_datetime(x_poly, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2503_copy['Record Time'], filtered_bsl_2503_copy['Calculated Distance (m)'], '.', label=\"Distance 2504-2503\", color='blue')\n",
    "\n",
    "# Polynomial fit\n",
    "plt.plot(x_poly_dates, y_poly, label=\"Quadratic Fit\", color='red')\n",
    "\n",
    "plt.title(\"Distance 2504-2503 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Centered Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "filtered_bsl_2503_copy['Record Time'] = pd.to_datetime(filtered_bsl_2503_copy['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "filtered_bsl_2503_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Sort by time for consistency\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_copy.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index for rolling calculation\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_copy.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day moving average\n",
    "filtered_bsl_2503_copy['Calculated Distance Moving Average'] = (\n",
    "    filtered_bsl_2503_copy['Calculated Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Reset index to keep 'Record Time' as a column\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_copy.reset_index()\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2503_copy['Record Time Numeric'] = filtered_bsl_2503_copy['Record Time'].astype(np.int64) // 10**9\n",
    "\n",
    "# Fit a cubic polynomial (degree 3)\n",
    "coefficients = np.polyfit(\n",
    "    filtered_bsl_2503_copy['Record Time Numeric'], \n",
    "    filtered_bsl_2503_copy['Calculated Distance (m)'], \n",
    "    3\n",
    ")\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Generate data points for the polynomial line\n",
    "x_poly = np.linspace(filtered_bsl_2503_copy['Record Time Numeric'].min(), \n",
    "                      filtered_bsl_2503_copy['Record Time Numeric'].max(), \n",
    "                      100)\n",
    "y_poly = polynomial(x_poly)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_poly_dates = pd.to_datetime(x_poly, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2503_copy['Record Time'], \n",
    "         filtered_bsl_2503_copy['Calculated Distance (m)']*100, \n",
    "         '.', label=\"Distance 2504-2503\", color='salmon', alpha=0.5)\n",
    "\n",
    "# Moving average\n",
    "plt.plot(filtered_bsl_2503_copy['Record Time'], \n",
    "         filtered_bsl_2503_copy['Calculated Distance Moving Average']*100,'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "# Polynomial fit\n",
    "#plt.plot(x_poly_dates, y_poly, label=\"Cubic Fit\", color='red', linewidth=2)\n",
    "\n",
    "plt.title(\"Distance west-central Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "#plt.ylim([1765.4,1765.9])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2503_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2503_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2503_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2503_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2503_copy['Record Time Numeric'], filtered_bsl_2503_copy['Calculated Distance (m)'], 2)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate the polynomial values\n",
    "y_poly = polynomial(filtered_bsl_2503_copy['Record Time Numeric'])\n",
    "\n",
    "# Calculate residuals (misfits)\n",
    "residuals = filtered_bsl_2503_copy['Calculated Distance (m)'] - y_poly\n",
    "\n",
    "# Plotting the histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, color='red', alpha=0.7)\n",
    "plt.title(\"Histogram of Residuals from Polynomial Fit for 2504-2503\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DataFrames based on the 'RangeAddress' column values\n",
    "bsl_df = df_dict['2503']['BSL']\n",
    "bsl_df['RangeAddress'] = bsl_df['RangeAddress'].astype(int).astype(str)\n",
    "bsl_df['Range(ms)'] = pd.to_numeric(bsl_df['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2502_df = bsl_df[bsl_df['RangeAddress'] == '2502'].copy()\n",
    "bsl_2504_df = bsl_df[bsl_df['RangeAddress'] == '2504'].copy()\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2502_df['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2502_df['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_bsl_2502_df = bsl_2502_df[(bsl_2502_df['Range(ms)'] >= lower_bound) & (bsl_2502_df['Range(ms)'] <= upper_bound)]\n",
    "outside_values_df1 = bsl_2502_df[(bsl_2502_df['Range(ms)'] < lower_bound) | (bsl_2502_df['Range(ms)'] > upper_bound)]\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2502_df['Range(ms)'] = pd.to_numeric(bsl_2502_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2504_df['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2504_df['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "\n",
    "# Filter out data points with 'Range(ms)' values using IQR\n",
    "filtered_bsl_2504_df = bsl_2504_df[(bsl_2504_df['Range(ms)'] >= lower_bound) & (bsl_2504_df['Range(ms)'] <= upper_bound)]\n",
    "outside_values_df2 = bsl_2504_df[(bsl_2504_df['Range(ms)'] < lower_bound) | (bsl_2504_df['Range(ms)'] > upper_bound)]\n",
    "\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2504_df['Range(ms)'] = pd.to_numeric(bsl_2504_df['Range(ms)'], errors='coerce')\n",
    "\n",
    "# Creating explicit copies of the DataFrames to avoid the warning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "filtered_bsl_2504_copy = filtered_bsl_2504_df.copy()\n",
    "\n",
    "# Create explicit copies to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2502_copy = filtered_bsl_2502_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2502_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2503'], 'Record Time')\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2502_copy timestamps\n",
    "filtered_bsl_2502_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Calculated Distance (m)'] = filtered_bsl_2502_copy['Interpolated Sound Speed'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "# Display the top rows of the updated dataframe\n",
    "filtered_bsl_2502_copy[['Record Time', 'Range(ms)', 'Interpolated Sound Speed', 'Calculated Distance (m)']].head()\n",
    "\n",
    "##############\n",
    "filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2502_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2502_2504']['Harmonic Mean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2502_copy['Harmonic Distance (m)'] = filtered_bsl_2502_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2502_copy['Range(ms)']-filtered_bsl_2502_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "##############\n",
    "west_east=filtered_bsl_2502_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy is defined and has necessary columns\n",
    "\n",
    "# Prepare data for plotting (convert to Unix timestamp and drop NaNs)\n",
    "filtered_bsl_2502_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2502_copy['Record Time']).astype(np.int64) // 10**9\n",
    "filtered_bsl_2502_copy.dropna(subset=['Calculated Distance (m)', 'Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit quadratic and linear polynomials for 'Calculated Distance (m)'\n",
    "coefficients_poly_calc = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 2)\n",
    "coefficients_linear_calc = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 1)\n",
    "polynomial_calc = np.poly1d(coefficients_poly_calc)\n",
    "linear_calc = np.poly1d(coefficients_linear_calc)\n",
    "\n",
    "# Fit quadratic and linear polynomials for 'Harmonic Distance (m)'\n",
    "coefficients_poly_harmonic = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Harmonic Distance (m)'], 2)\n",
    "coefficients_linear_harmonic = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Harmonic Distance (m)'], 1)\n",
    "polynomial_harmonic = np.poly1d(coefficients_poly_harmonic)\n",
    "linear_harmonic = np.poly1d(coefficients_linear_harmonic)\n",
    "\n",
    "# Generate data points for the polynomial and linear lines\n",
    "x_values = np.linspace(filtered_bsl_2502_copy['Record Time Numeric'].min(), filtered_bsl_2502_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly_calc = polynomial_calc(x_values)\n",
    "y_linear_calc = linear_calc(x_values)\n",
    "y_poly_harmonic = polynomial_harmonic(x_values)\n",
    "y_linear_harmonic = linear_harmonic(x_values)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_dates = pd.to_datetime(x_values, unit='s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals and standard deviation for Calculated Distance\n",
    "filtered_bsl_2502_copy['Residuals Calc'] = filtered_bsl_2502_copy['Calculated Distance (m)'] - polynomial_calc(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "std_dev_calc = filtered_bsl_2502_copy['Residuals Calc'].std()\n",
    "\n",
    "# Filter out outliers for Calculated Distance\n",
    "filtered_bsl_2502_no_outliers_calc = filtered_bsl_2502_copy[abs(filtered_bsl_2502_copy['Residuals Calc']) <= 3 * std_dev_calc]\n",
    "\n",
    "# Calculate residuals and standard deviation for Harmonic Distance\n",
    "filtered_bsl_2502_copy['Residuals Harmonic'] = filtered_bsl_2502_copy['Harmonic Distance (m)'] - polynomial_harmonic(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "std_dev_harmonic = filtered_bsl_2502_copy['Residuals Harmonic'].std()\n",
    "\n",
    "# Filter out outliers for Harmonic Distance\n",
    "filtered_bsl_2502_no_outliers_harmonic = filtered_bsl_2502_copy[abs(filtered_bsl_2502_copy['Residuals Harmonic']) <= 3 * std_dev_harmonic]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_no_outliers_calc['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "filtered_bsl_2502_no_outliers_calc['Record Time'] = pd.to_datetime(filtered_bsl_2502_no_outliers_calc['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "filtered_bsl_2502_no_outliers_calc.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Sort by time for consistency\n",
    "filtered_bsl_2502_no_outliers_calc = filtered_bsl_2502_no_outliers_calc.sort_values(by='Record Time')\n",
    "\n",
    "# Set 'Record Time' as the index for rolling calculation\n",
    "filtered_bsl_2502_no_outliers_calc = filtered_bsl_2502_no_outliers_calc.set_index('Record Time')\n",
    "\n",
    "# Compute 30-day moving average\n",
    "filtered_bsl_2502_no_outliers_calc['Calculated Distance Moving Average'] = (\n",
    "    filtered_bsl_2502_no_outliers_calc['Calculated Distance (m)']\n",
    "    .rolling('30D')\n",
    "    .mean()\n",
    ") * 100  # Multiply by 100 to match original scaling\n",
    "\n",
    "# Reset index to keep 'Record Time' as a column\n",
    "filtered_bsl_2502_no_outliers_calc = filtered_bsl_2502_no_outliers_calc.reset_index()\n",
    "plt.figure(figsize=(12, 7))\n",
    "# Plot original data\n",
    "plt.plot(filtered_bsl_2502_no_outliers_calc['Record Time'], \n",
    "         filtered_bsl_2502_no_outliers_calc['Calculated Distance (m)'] * 100, \n",
    "         '.', label=\"TEOS-10 Distance\", color='salmon', alpha=0.5)\n",
    "\n",
    "# Plot moving average\n",
    "plt.plot(filtered_bsl_2502_no_outliers_calc['Record Time'], \n",
    "         filtered_bsl_2502_no_outliers_calc['Calculated Distance Moving Average'],'r.', \n",
    "         label=\"30-day Moving Average\")\n",
    "\n",
    "plt.title(\"TEOS-10 Distance Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "#plt.ylim([326010,326030])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting with outliers removed\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# Data for Calculated Distance without outliers\n",
    "plt.plot(filtered_bsl_2502_no_outliers_calc['Record Time'], filtered_bsl_2502_no_outliers_calc['Calculated Distance (m)']*100, '.', label=\"TEOS-10 Distance\", color='grey')\n",
    "\n",
    "# Data for Harmonic Distance without outliers\n",
    "#plt.plot(filtered_bsl_2502_no_outliers_harmonic['Record Time'], filtered_bsl_2502_no_outliers_harmonic['Harmonic Distance (m)'], '.', label=\"Velocimeter Distance\", color='grey')\n",
    "\n",
    "# Polynomial fit for Calculated Distance\n",
    "plt.plot(x_dates, y_poly_calc*100, label=\"Quadratic Fit - TEOS10\", color='black',linewidth=3)\n",
    "\n",
    "# Polynomial fit for Harmonic Distance\n",
    "#plt.plot(x_dates, y_poly_harmonic, label=\"Quadratic Fit - Velocimeter\", color='black')\n",
    "\n",
    "# Linear fit for Calculated Distance\n",
    "#plt.plot(x_dates, y_linear_calc, label=\"Linear Fit - TEOS10\", color='black')\n",
    "\n",
    "# Linear fit for Harmonic Distance\n",
    "#plt.plot(x_dates, y_linear_harmonic, label=\"Linear Fit - Velocimeter\", color='purple')\n",
    "\n",
    "plt.title(\"Distance 2503-2502 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an explicit copy of filtered_bsl_2503_df to avoid the SettingWithCopyWarning\n",
    "filtered_bsl_2504_copy = filtered_bsl_2504_df.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2504_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_mean_dfs['2503_2504'], 'Record Time')\n",
    "\n",
    "# Interpolate sound speed values onto the filtered_bsl_2503_copy timestamps\n",
    "filtered_bsl_2504_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2504_copy['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2503_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2503_2504']['HMean']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2504_copy['Calculated Distance (m)'] = filtered_bsl_2504_copy['Interpolated Sound Speed'] * ((filtered_bsl_2504_copy['Range(ms)']-filtered_bsl_2504_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "\n",
    "##############\n",
    "filtered_bsl_2504_copy['Interpolated Harmonic Velocity'] = np.interp(\n",
    "    filtered_bsl_2504_copy['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2503_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_mean_dfs['2503_2504']['SoundSpeed']\n",
    ")\n",
    "\n",
    "# Calculate distance using the interpolated sound speeds\n",
    "filtered_bsl_2504_copy['Harmonic Distance (m)'] = filtered_bsl_2504_copy['Interpolated Harmonic Velocity'] * ((filtered_bsl_2504_copy['Range(ms)']-filtered_bsl_2504_copy['TAT(ms)']) / 2000)  # Dividing by 2000 to convert ms to seconds and account for two-way travel\n",
    "##############\n",
    "\n",
    "# Calculate centered distances by subtracting the mean\n",
    "centered_distance_2503_2502 = filtered_bsl_2502_copy['Calculated Distance (m)'] - filtered_bsl_2502_copy['Calculated Distance (m)'].mean()\n",
    "centered_distance_2503_2504 = filtered_bsl_2504_copy['Calculated Distance (m)'] - filtered_bsl_2504_copy['Calculated Distance (m)'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2502_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2502_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2502_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Generate data points for the polynomial line\n",
    "x_poly = np.linspace(filtered_bsl_2502_copy['Record Time Numeric'].min(), filtered_bsl_2502_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_poly)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_poly_dates = pd.to_datetime(x_poly, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2502_copy['Record Time'], filtered_bsl_2502_copy['Calculated Distance (m)'], '.', label=\"Distance 2503-2502\", color='blue')\n",
    "\n",
    "# Polynomial fit\n",
    "plt.plot(x_poly_dates, y_poly, label=\"Quadratic Fit\", color='red')\n",
    "\n",
    "plt.title(\"Distance 2503-2502 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2502_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2502_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2502_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2502_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2502_copy['Record Time Numeric'], filtered_bsl_2502_copy['Calculated Distance (m)'], 2)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate the polynomial values\n",
    "y_poly = polynomial(filtered_bsl_2502_copy['Record Time Numeric'])\n",
    "\n",
    "# Calculate residuals (misfits)\n",
    "residuals = filtered_bsl_2502_copy['Calculated Distance (m)'] - y_poly\n",
    "\n",
    "# Plotting the histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, color='blue', alpha=0.7)\n",
    "plt.title(\"Histogram of Residuals from Polynomial Fit for 2503-2502\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2502_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2503_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2504_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2504_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2504_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2504_copy['Record Time Numeric'], filtered_bsl_2504_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Generate data points for the polynomial line\n",
    "x_poly = np.linspace(filtered_bsl_2504_copy['Record Time Numeric'].min(), filtered_bsl_2504_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_poly)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_poly_dates = pd.to_datetime(x_poly, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2504_copy['Record Time'], filtered_bsl_2504_copy['Calculated Distance (m)']*100, '.', label=\"Distance 2503-2504\", color='grey')\n",
    "\n",
    "# Polynomial fit\n",
    "plt.plot(x_poly_dates, y_poly*100, label=\"Quadratic Fit\", color='black',linewidth=3)\n",
    "\n",
    "plt.title(\"Distance 2503-2504 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2504_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2504_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2504_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2504_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2504_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2504_copy['Record Time Numeric'], filtered_bsl_2504_copy['Calculated Distance (m)'], 2)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate the polynomial values\n",
    "y_poly = polynomial(filtered_bsl_2504_copy['Record Time Numeric'])\n",
    "\n",
    "# Calculate residuals (misfits)\n",
    "residuals = filtered_bsl_2504_copy['Calculated Distance (m)'] - y_poly\n",
    "\n",
    "# Plotting the histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, color='green', alpha=0.7)\n",
    "plt.title(\"Histogram of Residuals from Polynomial Fit for 2503-2504\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2504_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract DataFrames based on the 'RangeAddress' column values\n",
    "bsl_dfs = df_dict['2502']['BSL']\n",
    "bsl_dfs['RangeAddress'] = bsl_dfs['RangeAddress'].astype(int).astype(str)\n",
    "bsl_dfs['Range(ms)'] = pd.to_numeric(bsl_dfs['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2503_dfs = bsl_dfs[bsl_dfs['RangeAddress'] == '2503'].copy()\n",
    "bsl_2504_dfs = bsl_dfs[bsl_dfs['RangeAddress'] == '2504'].copy()\n",
    "\n",
    "# Calculate the IQR for the 'Range(ms)' column in bsl_2502_df\n",
    "Q1 = bsl_2503_dfs['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2503_dfs['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_bsl_2503_dfs = bsl_2503_dfs[(bsl_2503_dfs['Range(ms)'] >= 4636.2) & (bsl_2503_dfs['Range(ms)'] <= 4638)]\n",
    "outside_values_df2 = bsl_2503_dfs[(bsl_2503_dfs['Range(ms)'] < 4636.2) & (bsl_2503_dfs['Range(ms)'] > 4638)]\n",
    "\n",
    "# Convert the 'Range(ms)' column to numeric for plotting\n",
    "bsl_2503_dfs['Range(ms)'] = pd.to_numeric(bsl_2503_dfs['Range(ms)'], errors='coerce')\n",
    "bsl_2503_dfs['TAT(ms)'] = pd.to_numeric(bsl_2503_dfs['TAT(ms)'], errors='coerce')\n",
    "Q1 = bsl_2504_dfs['Range(ms)'].quantile(0.25)\n",
    "Q3 = bsl_2504_dfs['Range(ms)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "filtered_bsl_2504_dfs = bsl_2504_dfs[(bsl_2504_dfs['Range(ms)'] >= 2400)]\n",
    "outside_values_df2 = bsl_2504_dfs[(bsl_2504_dfs['Range(ms)'] < 2400)]\n",
    "\n",
    "bsl_2504_dfs['Range(ms)'] = pd.to_numeric(bsl_2504_dfs['Range(ms)'], errors='coerce')\n",
    "bsl_2504_dfs['TAT(ms)'] = pd.to_numeric(bsl_2504_dfs['TAT(ms)'], errors='coerce')\n",
    "\n",
    "filtered_bsl_2503_copy = filtered_bsl_2503_dfs.copy()\n",
    "filtered_bsl_2504_copy = filtered_bsl_2504_dfs.copy()\n",
    "\n",
    "ensure_datetime(filtered_bsl_2503_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_mean_dfs['2502_2503'], 'Record Time')\n",
    "\n",
    "# Interpolation for 2503\n",
    "filtered_bsl_2503_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2503_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2503']['HMean']\n",
    ")\n",
    "\n",
    "filtered_bsl_2503_copy['Calculated Distance (m)'] = filtered_bsl_2503_copy['Interpolated Sound Speed'] * ((filtered_bsl_2503_copy['Range(ms)']-pd.to_numeric(filtered_bsl_2503_copy['TAT(ms)'])) / 2000)\n",
    "\n",
    "\n",
    "# Interpolation for 2504\n",
    "ensure_datetime(filtered_bsl_2504_copy, 'Record Time')\n",
    "ensure_datetime(harmonic_df_dict['2502_2504'], 'Record Time')\n",
    "\n",
    "filtered_bsl_2504_copy['Interpolated Sound Speed'] = np.interp(\n",
    "    filtered_bsl_2504_copy['Record Time'].astype(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['Record Time'].view(np.int64),\n",
    "    harmonic_df_dict['2502_2504']['HMean']\n",
    ")\n",
    "\n",
    "filtered_bsl_2504_copy['Calculated Distance (m)'] = filtered_bsl_2504_copy['Interpolated Sound Speed'] * ((filtered_bsl_2504_copy['Range(ms)']-pd.to_numeric(filtered_bsl_2504_copy['TAT(ms)'])) / 2000)\n",
    "\n",
    "# Calculate centered distances\n",
    "#centered_distance_2502_2503 = filtered_bsl_2503_copy['Calculated Distance (m)'] - filtered_bsl_2503_copy['Calculated Distance (m)'].mean()\n",
    "#centered_distance_2502_2504 = filtered_bsl_2504_copy['Calculated Distance (m)'] - filtered_bsl_2504_copy['Calculated Distance (m)'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2503_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2503_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2503_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2503_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2503_copy['Record Time Numeric'], filtered_bsl_2503_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Generate data points for the polynomial line\n",
    "x_poly = np.linspace(filtered_bsl_2503_copy['Record Time Numeric'].min(), filtered_bsl_2503_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_poly)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_poly_dates = pd.to_datetime(x_poly, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2503_copy['Record Time'], filtered_bsl_2503_copy['Calculated Distance (m)'], '.', label=\"Distance 2502-2503\", color='blue')\n",
    "\n",
    "# Polynomial fit\n",
    "plt.plot(x_poly_dates, y_poly, label=\"Quadratic Fit\", color='red')\n",
    "\n",
    "plt.title(\"Distance 2502-2503 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2503_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2503_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2503_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2503_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2503_copy['Record Time Numeric'], filtered_bsl_2503_copy['Calculated Distance (m)'], 2)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate the polynomial values\n",
    "y_poly = polynomial(filtered_bsl_2503_copy['Record Time Numeric'])\n",
    "\n",
    "# Calculate residuals (misfits)\n",
    "residuals = filtered_bsl_2503_copy['Calculated Distance (m)'] - y_poly\n",
    "\n",
    "# Plotting the histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, color='red', alpha=0.7)\n",
    "plt.title(\"Histogram of Residuals from Polynomial Fit for 2502-2503\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bsl_2503_copy['Calculated Distance (m)'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2503_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2504_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2504_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2504_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2504_copy['Record Time Numeric'], filtered_bsl_2504_copy['Calculated Distance (m)'], 3)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Generate data points for the polynomial line\n",
    "x_poly = np.linspace(filtered_bsl_2504_copy['Record Time Numeric'].min(), filtered_bsl_2504_copy['Record Time Numeric'].max(), 100)\n",
    "y_poly = polynomial(x_poly)\n",
    "\n",
    "# Convert numeric time back to datetime for plotting\n",
    "x_poly_dates = pd.to_datetime(x_poly, unit='s')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Original data\n",
    "plt.plot(filtered_bsl_2504_copy['Record Time'], filtered_bsl_2504_copy['Calculated Distance (m)'], '.', label=\"Distance 2502-2504\", color='blue')\n",
    "\n",
    "# Polynomial fit\n",
    "plt.plot(x_poly_dates, y_poly, label=\"Quadratic Fit\", color='red')\n",
    "\n",
    "plt.title(\"Distance 2502-2504 Timeseries\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming filtered_bsl_2504_copy is defined and has 'Record Time' and 'Calculated Distance (m)'\n",
    "\n",
    "# Convert 'Record Time' to Unix timestamp (numeric) for fitting\n",
    "filtered_bsl_2504_copy['Record Time Numeric'] = pd.to_datetime(filtered_bsl_2504_copy['Record Time']).astype(np.int64) // 10**9\n",
    "\n",
    "# Remove any NaN values\n",
    "filtered_bsl_2504_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Fit a quadratic polynomial (2nd degree)\n",
    "coefficients = np.polyfit(filtered_bsl_2504_copy['Record Time Numeric'], filtered_bsl_2504_copy['Calculated Distance (m)'], 2)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate the polynomial values\n",
    "y_poly = polynomial(filtered_bsl_2504_copy['Record Time Numeric'])\n",
    "\n",
    "# Calculate residuals (misfits)\n",
    "residuals = filtered_bsl_2504_copy['Calculated Distance (m)'] - y_poly\n",
    "\n",
    "# Plotting the histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, color='green', alpha=0.7)\n",
    "plt.title(\"Histogram of Residuals from Polynomial Fit for 2502-2504\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_df = df_dict['2502']['BSL']\n",
    "bsl_df['RangeAddress'] = bsl_df['RangeAddress'].astype(int).astype(str)\n",
    "bsl_df['Range(ms)'] = pd.to_numeric(bsl_df['Range(ms)'], errors='coerce')\n",
    "bsl_df['TAT(ms)'] = pd.to_numeric(bsl_df['TAT(ms)'], errors='coerce')\n",
    "bsl_2503_df = bsl_df[bsl_df['RangeAddress'] == '2503'].copy()\n",
    "bsl_2503_df['Record Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(pd.to_datetime(bsl_2503_df['Record Time']),bsl_2503_df['Range(ms)'],s=1)\n",
    "plt.title('Two way travel time from 2502-2503 ')\n",
    "plt.xlabel('Record Time')\n",
    "plt.ylabel('Range (ms)')\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hypothetical dataframes: filtered_bsl_2504_dfs and bsl_2503_df\n",
    "# Both dataframes have a column 'Range(ms)'\n",
    "\n",
    "# Finding the range of data in both datasets\n",
    "min_range = min(filtered_bsl_2504_dfs['Range(ms)'].min(), bsl_2503_df['Range(ms)'].min())\n",
    "max_range = max(filtered_bsl_2504_dfs['Range(ms)'].max(), bsl_2503_df['Range(ms)'].max())\n",
    "\n",
    "# Creating 30 bins within this range\n",
    "bins = np.linspace(min_range, max_range, 31)\n",
    "\n",
    "# Creating the histograms with the same bins for both datasets\n",
    "plt.hist(filtered_bsl_2504_dfs['Range(ms)'], bins, alpha=0.5, label='filtered_bsl_2504_dfs')\n",
    "plt.hist(bsl_2503_df['Range(ms)'], bins, alpha=0.5, label='bsl_2503_df')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Range(ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histograms with Same Bin Width')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "filtered_bsl_2502_no_outliers_calc['Record Time'] = pd.to_datetime(filtered_bsl_2502_no_outliers_calc['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "filtered_bsl_2502_no_outliers_calc.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = filtered_bsl_2502_no_outliers_calc.resample('30D', on='Record Time').agg({\n",
    "    'Calculated Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean values to cm to match original scaling\n",
    "binned_data['Mean Distance'] *= 100\n",
    "binned_data['Std Dev'] *= 100\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"TEOS-10 Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "filtered_bsl_2503_copy['Record Time'] = pd.to_datetime(filtered_bsl_2503_copy['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "filtered_bsl_2503_copy.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data = filtered_bsl_2503_copy.resample('30D', on='Record Time').agg({\n",
    "    'Calculated Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='salmon', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"Central-West Distance (30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "west_east['Record Time'] = pd.to_datetime(west_east['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "west_east.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins and calculate mean and standard deviation\n",
    "binned_data =west_east.resample('30D', on='Record Time').agg({\n",
    "    'Calculated Distance (m)': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean Distance', 'Std Dev']\n",
    "\n",
    "# Convert index (which is now the start of each 30-day bin) to a column for plotting\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Plot the binned mean values with error bars (std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean Distance'], \n",
    "    yerr=binned_data['Std Dev'], \n",
    "    fmt='o-', color='blue', label=\"30-day Binned Mean (±1σ)\", capsize=5\n",
    ")\n",
    "\n",
    "plt.title(\"West-east(30-Day Binned) with Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (m)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2504_2502['Record Time'] = pd.to_datetime(R2504_2502['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Calculated Distance (m)'\n",
    "R2504_2502.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins; calculate mean, median, and std\n",
    "binned_data = R2504_2502.resample('30D', on='Record Time').agg({\n",
    "    'Calculated Distance (m)': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean (m)', 'Median (m)', 'Std Dev (m)']\n",
    "\n",
    "# Reset index to make the time bin starts into a column\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert mean, median, std to cm\n",
    "binned_data['Mean (cm)'] = binned_data['Mean (m)'] * 100\n",
    "binned_data['Median (cm)'] = binned_data['Median (m)'] * 100\n",
    "binned_data['Std Dev (cm)'] = binned_data['Std Dev (m)'] * 100\n",
    "\n",
    "# Plot both mean and median with error bars\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Mean (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='o-',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Mean (±1σ)\"\n",
    ")\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Median (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='s--',  # different marker/line style to distinguish from mean\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Median (±1σ)\"\n",
    ")\n",
    "\n",
    "plt.title(\"Central-East Distance (30-Day Binned) with Mean and Median + Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'Record Time' to datetime if needed\n",
    "R2504_2502['Record Time'] = pd.to_datetime(R2504_2502['Record Time'])\n",
    "\n",
    "# Remove NaN values\n",
    "R2504_2502.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute overall mean and median (in cm)\n",
    "mean_val = R2504_2502['Calculated Distance (m)'].mean() * 100\n",
    "median_val = R2504_2502['Calculated Distance (m)'].median() * 100\n",
    "\n",
    "# Convert distances to cm\n",
    "distances_cm = R2504_2502['Calculated Distance (m)'] * 100\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_mean = distances_cm - mean_val\n",
    "residuals_median = distances_cm - median_val\n",
    "\n",
    "# Plot the two sets of residuals on the same histogram\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(\n",
    "    residuals_mean, \n",
    "    bins=30, \n",
    "    alpha=0.5, \n",
    "    label='Residual: Data - Mean'\n",
    ")\n",
    "plt.hist(\n",
    "    residuals_median, \n",
    "    bins=30, \n",
    "    alpha=0.5, \n",
    "    label='Residual: Data - Median'\n",
    ")\n",
    "\n",
    "# Optional: Add a line at x=0 to show zero residual\n",
    "plt.axvline(x=0, linestyle='--', color='k', label=\"Zero Residual\")\n",
    "\n",
    "plt.title(\"Histogram of Residuals: Data vs. Overall Mean and Median: Central-East\")\n",
    "plt.xlabel(\"Residual (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2504['Record Time'] = pd.to_datetime(R2503_2504['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Harmonic Distance (m)'\n",
    "R2503_2504.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins; calculate mean, median, and std dev\n",
    "binned_data = R2503_2504.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean (m)', 'Median (m)', 'Std Dev (m)']\n",
    "\n",
    "# Convert index (start of each 30-day bin) to a column\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert to centimeters\n",
    "binned_data['Mean (cm)'] = binned_data['Mean (m)'] * 100\n",
    "binned_data['Median (cm)'] = binned_data['Median (m)'] * 100\n",
    "binned_data['Std Dev (cm)'] = binned_data['Std Dev (m)'] * 100\n",
    "\n",
    "# Plot both mean and median with error bars (±1 std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Mean (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='o-',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Mean (±1σ)\"\n",
    ")\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Median (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='s--',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Median (±1σ)\"\n",
    ")\n",
    "\n",
    "plt.title(\"West-Central Distance (30-Day Binned) with Mean and Median + Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2504['Record Time'] = pd.to_datetime(R2503_2504['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Harmonic Distance (m)'\n",
    "R2503_2504.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute overall mean and median (convert to cm)\n",
    "mean_val = R2503_2504['Harmonic Distance (m)'].mean() * 100\n",
    "median_val = R2503_2504['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Convert all distances to cm\n",
    "distances_cm = R2503_2504['Harmonic Distance (m)'] * 100\n",
    "\n",
    "# Calculate residuals relative to the overall mean and median\n",
    "residuals_mean = distances_cm - mean_val\n",
    "residuals_median = distances_cm - median_val\n",
    "\n",
    "# Plot residuals for mean and median in one histogram (two overlays)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(\n",
    "    residuals_mean,\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    label='Residuals: Data - Mean'\n",
    ")\n",
    "plt.hist(\n",
    "    residuals_median,\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    label='Residuals: Data - Median'\n",
    ")\n",
    "\n",
    "# Optional: draw a vertical line at zero to highlight no residual\n",
    "plt.axvline(x=0, linestyle='--', color='k', label='Zero Residual')\n",
    "\n",
    "plt.title(\"Histogram of Residuals (Data vs. Overall Mean & Median)\\nWest-Central Distance\")\n",
    "plt.xlabel(\"Residual (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2503_2502['Record Time'] = pd.to_datetime(R2503_2502['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Harmonic Distance (m)'\n",
    "R2503_2502.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins; calculate mean, median, and std dev\n",
    "binned_data = R2503_2502.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean (m)', 'Median (m)', 'Std Dev (m)']\n",
    "\n",
    "# Reset index to make the time bins a column\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert to centimeters\n",
    "binned_data['Mean (cm)'] = binned_data['Mean (m)'] * 100\n",
    "binned_data['Median (cm)'] = binned_data['Median (m)'] * 100\n",
    "binned_data['Std Dev (cm)'] = binned_data['Std Dev (m)'] * 100\n",
    "\n",
    "# Plot both mean and median with error bars (±1 std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Mean (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='o-',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Mean (±1σ)\"\n",
    ")\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Median (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='s--',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Median (±1σ)\"\n",
    ")\n",
    "\n",
    "plt.title(\"West-East Distance (30-Day Binned) with Mean and Median + Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format (if not already done)\n",
    "R2503_2502['Record Time'] = pd.to_datetime(R2503_2502['Record Time'])\n",
    "\n",
    "# Remove NaN values in 'Harmonic Distance (m)'\n",
    "R2503_2502.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute the overall mean and median (convert to cm)\n",
    "mean_val = R2503_2502['Harmonic Distance (m)'].mean() * 100\n",
    "median_val = R2503_2502['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Convert all distances to cm\n",
    "distances_cm = R2503_2502['Harmonic Distance (m)'] * 100\n",
    "\n",
    "# Calculate residuals relative to the overall mean and median\n",
    "residuals_mean = distances_cm - mean_val\n",
    "residuals_median = distances_cm - median_val\n",
    "\n",
    "# Plot histogram for both sets of residuals\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(residuals_mean, bins=30, alpha=0.5, label='Residuals: Data - Mean')\n",
    "plt.hist(residuals_median, bins=30, alpha=0.5, label='Residuals: Data - Median')\n",
    "\n",
    "# Optional: add a vertical line at zero\n",
    "plt.axvline(x=0, linestyle='--', color='k', label=\"Zero Residual\")\n",
    "\n",
    "plt.title(\"Histogram of Residuals (Data vs. Overall Mean & Median)\\nWest-East Distance\")\n",
    "plt.xlabel(\"Residual (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 1) R2504_2502 Dataset\n",
    "# ---------------------------\n",
    "df_ce = R2504_2502.copy()  # e.g., \"Central-East\"\n",
    "df_ce['Record Time'] = pd.to_datetime(df_ce['Record Time'])\n",
    "df_ce.dropna(subset=['Calculated Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute overall median (in cm)\n",
    "overall_median_ce = df_ce['Calculated Distance (m)'].median() * 100\n",
    "\n",
    "# Compute 30-day binned median\n",
    "df_ce_binned = df_ce.resample('30D', on='Record Time').agg({\n",
    "    'Calculated Distance (m)': 'median'\n",
    "}).reset_index()\n",
    "\n",
    "# Convert the binned median to cm\n",
    "df_ce_binned['Median (cm)'] = df_ce_binned['Calculated Distance (m)'] * 100\n",
    "\n",
    "# Subtract overall median to get \"difference from median\"\n",
    "df_ce_binned['Diff from Overall Median (cm)'] = (\n",
    "    df_ce_binned['Median (cm)'] - overall_median_ce\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) R2503_2504 Dataset\n",
    "# ---------------------------\n",
    "df_wc = R2503_2504.copy()  # e.g., \"West-Central\"\n",
    "df_wc['Record Time'] = pd.to_datetime(df_wc['Record Time'])\n",
    "df_wc.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute overall median (in cm)\n",
    "overall_median_wc = df_wc['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Compute 30-day binned median\n",
    "df_wc_binned = df_wc.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': 'median'\n",
    "}).reset_index()\n",
    "\n",
    "df_wc_binned['Median (cm)'] = df_wc_binned['Harmonic Distance (m)'] * 100\n",
    "\n",
    "df_wc_binned['Diff from Overall Median (cm)'] = (\n",
    "    df_wc_binned['Median (cm)'] - overall_median_wc\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) R2503_2502 Dataset\n",
    "# ---------------------------\n",
    "df_we = R2503_2502.copy()  # e.g., \"West-East\"\n",
    "df_we['Record Time'] = pd.to_datetime(df_we['Record Time'])\n",
    "df_we.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute overall median (in cm)\n",
    "overall_median_we = df_we['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Compute 30-day binned median\n",
    "df_we_binned = df_we.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': 'median'\n",
    "}).reset_index()\n",
    "\n",
    "df_we_binned['Median (cm)'] = df_we_binned['Harmonic Distance (m)'] * 100\n",
    "\n",
    "df_we_binned['Diff from Overall Median (cm)'] = (\n",
    "    df_we_binned['Median (cm)'] - overall_median_we\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot all three \"difference from median\" series on one figure\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(\n",
    "    df_ce_binned['Record Time'],\n",
    "    df_ce_binned['Diff from Overall Median (cm)'],\n",
    "    marker='o',\n",
    "    label='R2504_2502 (Central-East)'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    df_wc_binned['Record Time'],\n",
    "    df_wc_binned['Diff from Overall Median (cm)'],\n",
    "    marker='s',\n",
    "    label='R2503_2504 (West-Central)'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    df_we_binned['Record Time'],\n",
    "    df_we_binned['Diff from Overall Median (cm)'],\n",
    "    marker='^',\n",
    "    label='R2503_2502 (West-East)'\n",
    ")\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--', label='Overall Median Line')\n",
    "plt.title(\"30-Day Binned Median Minus Overall Median for Each Dataset\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Difference from Own Overall Median (cm)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2504_2503['Record Time'] = pd.to_datetime(R2504_2503['Record Time'])\n",
    "\n",
    "# Remove NaN values in 'Harmonic Distance (m)'\n",
    "R2504_2503.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins; calculate mean, median, and std dev\n",
    "binned_data = R2504_2503.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean (m)', 'Median (m)', 'Std Dev (m)']\n",
    "\n",
    "# Convert index (start of each 30-day bin) to a column\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert values to centimeters\n",
    "binned_data['Mean (cm)']   = binned_data['Mean (m)']   * 100\n",
    "binned_data['Median (cm)'] = binned_data['Median (m)'] * 100\n",
    "binned_data['Std Dev (cm)'] = binned_data['Std Dev (m)'] * 100\n",
    "\n",
    "# Plot both mean and median with error bars (±1 std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Mean (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='o-',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Mean (±1σ)\"\n",
    ")\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'],\n",
    "    binned_data['Median (cm)'],\n",
    "    yerr=binned_data['Std Dev (cm)'],\n",
    "    fmt='s--',\n",
    "    capsize=5,\n",
    "    label=\"30-day Binned Median (±1σ)\"\n",
    ")\n",
    "\n",
    "plt.title(\"Central-West Distance (30-Day Binned) with Mean and Median + Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format (if not already done)\n",
    "R2504_2503['Record Time'] = pd.to_datetime(R2504_2503['Record Time'])\n",
    "\n",
    "# Remove NaN values in 'Harmonic Distance (m)'\n",
    "R2504_2503.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Compute the overall mean and median (in cm)\n",
    "mean_val = R2504_2503['Harmonic Distance (m)'].mean() * 100\n",
    "median_val = R2504_2503['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Convert all distances to cm\n",
    "distances_cm = R2504_2503['Harmonic Distance (m)'] * 100\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_mean = distances_cm - mean_val\n",
    "residuals_median = distances_cm - median_val\n",
    "\n",
    "# Plot histogram of both residuals\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(residuals_mean, bins=30, alpha=0.5, label='Residuals: Data - Mean')\n",
    "plt.hist(residuals_median, bins=30, alpha=0.5, label='Residuals: Data - Median')\n",
    "\n",
    "# Optional: vertical line at zero\n",
    "plt.axvline(0, linestyle='--', color='k', label='Zero Residual')\n",
    "\n",
    "plt.title(\"Histogram of Residuals (Data vs. Overall Mean & Median)\\nCentral-West Distance\")\n",
    "plt.xlabel(\"Residual (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2504['Record Time'] = pd.to_datetime(R2502_2504['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Harmonic Distance (m)'\n",
    "R2502_2504.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins; calculate mean, median, and std dev\n",
    "binned_data = R2502_2504.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean (m)', 'Median (m)', 'Std Dev (m)']\n",
    "\n",
    "# Reset index to convert the time bins into a column\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert to centimeters\n",
    "binned_data['Mean (cm)']   = binned_data['Mean (m)']   * 100\n",
    "binned_data['Median (cm)'] = binned_data['Median (m)'] * 100\n",
    "binned_data['Std Dev (cm)'] = binned_data['Std Dev (m)'] * 100\n",
    "\n",
    "# Plot the binned mean and median with ±1σ error bars\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean (cm)'], \n",
    "    yerr=binned_data['Std Dev (cm)'], \n",
    "    fmt='o-', \n",
    "    capsize=5, \n",
    "    label=\"30-day Binned Mean (±1σ)\"\n",
    ")\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Median (cm)'], \n",
    "    yerr=binned_data['Std Dev (cm)'], \n",
    "    fmt='s--', \n",
    "    capsize=5, \n",
    "    label=\"30-day Binned Median (±1σ)\"\n",
    ")\n",
    "\n",
    "plt.title(\"East-Central Distance (30-Day Binned) with Mean & Median + Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2504['Record Time'] = pd.to_datetime(R2502_2504['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Harmonic Distance (m)'\n",
    "R2502_2504.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Calculate the overall mean and median (in cm)\n",
    "mean_val = R2502_2504['Harmonic Distance (m)'].mean() * 100\n",
    "median_val = R2502_2504['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Convert all distances to cm\n",
    "distances_cm = R2502_2504['Harmonic Distance (m)'] * 100\n",
    "\n",
    "# Compute residuals with respect to the overall mean and median\n",
    "residuals_mean = distances_cm - mean_val\n",
    "residuals_median = distances_cm - median_val\n",
    "\n",
    "# Plot histograms of the residuals\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(residuals_mean, bins=30, alpha=0.5, label='Residuals: Data - Mean')\n",
    "plt.hist(residuals_median, bins=30, alpha=0.5, label='Residuals: Data - Median')\n",
    "\n",
    "# Optional: add a vertical line at zero\n",
    "plt.axvline(x=0, linestyle='--', color='k', label='Zero Residual')\n",
    "\n",
    "plt.title(\"Histogram of Residuals (Data vs. Overall Mean & Median)\\nEast-Central Distance\")\n",
    "plt.xlabel(\"Residual (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Record Time' is in datetime format\n",
    "R2502_2503['Record Time'] = pd.to_datetime(R2502_2503['Record Time'])\n",
    "\n",
    "# Remove any NaN values in 'Harmonic Distance (m)'\n",
    "R2502_2503.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Group data into 30-day bins; calculate mean, median, and std dev\n",
    "binned_data = R2502_2503.resample('30D', on='Record Time').agg({\n",
    "    'Harmonic Distance (m)': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "binned_data.columns = ['Mean (m)', 'Median (m)', 'Std Dev (m)']\n",
    "\n",
    "# Convert the time bins (index) to a column\n",
    "binned_data = binned_data.reset_index()\n",
    "\n",
    "# Convert to centimeters\n",
    "binned_data['Mean (cm)']   = binned_data['Mean (m)']   * 100\n",
    "binned_data['Median (cm)'] = binned_data['Median (m)'] * 100\n",
    "binned_data['Std Dev (cm)'] = binned_data['Std Dev (m)'] * 100\n",
    "\n",
    "# Plot both mean and median with error bars (±1 std dev)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Mean (cm)'], \n",
    "    yerr=binned_data['Std Dev (cm)'], \n",
    "    fmt='o-', \n",
    "    capsize=5, \n",
    "    label=\"30-day Binned Mean (±1σ)\"\n",
    ")\n",
    "plt.errorbar(\n",
    "    binned_data['Record Time'], \n",
    "    binned_data['Median (cm)'], \n",
    "    yerr=binned_data['Std Dev (cm)'], \n",
    "    fmt='s--', \n",
    "    capsize=5, \n",
    "    label=\"30-day Binned Median (±1σ)\"\n",
    ")\n",
    "\n",
    "plt.title(\"East-West Distance (30-Day Binned) with Mean & Median + Error Bars\")\n",
    "plt.xlabel(\"Record Time\")\n",
    "plt.ylabel(\"Calculated Distance (cm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert 'Record Time' to datetime if needed\n",
    "R2502_2503['Record Time'] = pd.to_datetime(R2502_2503['Record Time'])\n",
    "\n",
    "# Remove NaN values in 'Harmonic Distance (m)'\n",
    "R2502_2503.dropna(subset=['Harmonic Distance (m)'], inplace=True)\n",
    "\n",
    "# Calculate overall mean and median, converting to cm\n",
    "mean_val = R2502_2503['Harmonic Distance (m)'].mean() * 100\n",
    "median_val = R2502_2503['Harmonic Distance (m)'].median() * 100\n",
    "\n",
    "# Convert all distances to cm\n",
    "distances_cm = R2502_2503['Harmonic Distance (m)'] * 100\n",
    "\n",
    "# Compute residuals\n",
    "residuals_mean = distances_cm - mean_val\n",
    "residuals_median = distances_cm - median_val\n",
    "\n",
    "# Plot the histogram of residuals for both mean and median\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.hist(residuals_mean, bins=30, alpha=0.5, label='Residuals: Data - Mean')\n",
    "plt.hist(residuals_median, bins=30, alpha=0.5, label='Residuals: Data - Median')\n",
    "\n",
    "# (Optional) Add a vertical line at zero\n",
    "plt.axvline(0, linestyle='--', color='k', label='Zero Residual')\n",
    "\n",
    "plt.title(\"Histogram of Residuals (Data vs. Overall Mean & Median)\\nEast-West Distance\")\n",
    "plt.xlabel(\"Residual (cm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Drop NaNs in each DataFrame\n",
    "R2502_2504_clean = R2502_2504.dropna(subset=['Harmonic Distance (m)'])  # East-Central\n",
    "R2503_2502_clean = R2503_2502.dropna(subset=['Harmonic Distance (m)'])  # East-West\n",
    "R2503_2504_clean = R2503_2504.dropna(subset=['Harmonic Distance (m)'])  # West-Central\n",
    "R2504_2503_clean = R2504_2503.dropna(subset=['Harmonic Distance (m)'])  # Central-West\n",
    "R2502_2503_clean = R2502_2503.dropna(subset=['Harmonic Distance (m)'])  # West-East\n",
    "R2504_2502_clean = R2504_2502.dropna(subset=['Harmonic Distance (m)'])  # Central-East\n",
    "\n",
    "# 2) Calculate mean in kilometers, std dev in centimeters, and precision (cm/km)\n",
    "distances = {\n",
    "    \"Central to East\": (R2504_2502_clean['Harmonic Distance (m)'].mean() / 1000, \n",
    "                        R2504_2502_clean['Harmonic Distance (m)'].std() * 100),\n",
    "    \"East to Central\": (R2502_2504_clean['Harmonic Distance (m)'].mean() / 1000, \n",
    "                        R2502_2504_clean['Harmonic Distance (m)'].std() * 100),\n",
    "    \"Central to West\": (R2504_2503_clean['Harmonic Distance (m)'].mean() / 1000, \n",
    "                        R2504_2503_clean['Harmonic Distance (m)'].std() * 100),\n",
    "    \"West to Central\": (R2503_2504_clean['Harmonic Distance (m)'].mean() / 1000, \n",
    "                        R2503_2504_clean['Harmonic Distance (m)'].std() * 100),\n",
    "    \"East to West\": (R2503_2502_clean['Harmonic Distance (m)'].mean() / 1000, \n",
    "                     R2503_2502_clean['Harmonic Distance (m)'].std() * 100),\n",
    "    \"West to East\": (R2502_2503_clean['Harmonic Distance (m)'].mean() / 1000, \n",
    "                     R2502_2503_clean['Harmonic Distance (m)'].std() * 100),\n",
    "}\n",
    "\n",
    "# 3) Print results with precision\n",
    "print(\"=== Mean Distances in km, Std Dev in cm, & Precision (cm/km) ===\\n\")\n",
    "for label, (mean_km, std_cm) in distances.items():\n",
    "    precision_cm_per_km = std_cm / mean_km  # Compute precision\n",
    "    print(f\"{label}: Mean = {mean_km:.4f} km, Std Dev = {std_cm:.4f} cm, Precision = {precision_cm_per_km:.2f} cm/km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfetch",
   "language": "python",
   "name": "pyfetch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
